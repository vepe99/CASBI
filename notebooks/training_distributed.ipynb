{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CASBI\n",
    "import CASBI.utils as utils\n",
    "from CASBI.generator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_dataframe = '../../data/dataframe/dataframe.parquet'\n",
    "test_and_nll_path = '../../data/test_and_nll/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home/vgiusepp/miniconda3/envs/test/lib/python3.10/site-packages/CASBI/distributed_training.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_time = pd.concat((df_time, df_mass_masked[df_mass_masked['Galaxy_name']==galaxy_temp]) )\n",
      "/export/home/vgiusepp/miniconda3/envs/test/lib/python3.10/site-packages/CASBI/distributed_training.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_time = pd.concat((df_time, df_mass_masked[df_mass_masked['Galaxy_name']==galaxy_temp]) )\n",
      "/export/home/vgiusepp/miniconda3/envs/test/lib/python3.10/site-packages/CASBI/distributed_training.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_time = pd.concat((df_time, df_mass_masked[df_mass_masked['Galaxy_name']==galaxy_temp]) )\n",
      "/export/home/vgiusepp/miniconda3/envs/test/lib/python3.10/site-packages/CASBI/distributed_training.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_time = pd.concat((df_time, df_mass_masked[df_mass_masked['Galaxy_name']==galaxy_temp]) )\n",
      "/export/home/vgiusepp/miniconda3/envs/test/lib/python3.10/site-packages/CASBI/distributed_training.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_time = pd.concat((df_time, df_mass_masked[df_mass_masked['Galaxy_name']==galaxy_temp]) )\n",
      "/export/home/vgiusepp/miniconda3/envs/test/lib/python3.10/site-packages/CASBI/distributed_training.py:209: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_time = pd.concat((df_time, df_mass_masked[df_mass_masked['Galaxy_name']==galaxy_temp]) )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish prepare data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/export/home/vgiusepp/miniconda3/envs/test/lib/python3.10/site-packages/CASBI/distributed_training.py\", line 293, in <module>\n",
      "[rank0]:     main(args.path_train_dataframe, args.test_and_nll_path, args.total_epochs, args.batch_size, args.snapshot_path,)\n",
      "[rank0]:   File \"/export/home/vgiusepp/miniconda3/envs/test/lib/python3.10/site-packages/CASBI/distributed_training.py\", line 273, in main\n",
      "[rank0]:     trainer = Trainer(model, train_data, val_data, test_data, optimizer, snapshot_path)\n",
      "[rank0]:   File \"/export/home/vgiusepp/miniconda3/envs/test/lib/python3.10/site-packages/CASBI/distributed_training.py\", line 74, in __init__\n",
      "[rank0]:     self.model = model.to(self.gpu_id)\n",
      "[rank0]:   File \"/export/home/vgiusepp/miniconda3/envs/test/lib/python3.10/site-packages/CASBI/generator.py\", line 553, in to\n",
      "[rank0]:     super().to(device)\n",
      "[rank0]:   File \"/export/home/vgiusepp/miniconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1173, in to\n",
      "[rank0]:     return self._apply(convert)\n",
      "[rank0]:   File \"/export/home/vgiusepp/miniconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 779, in _apply\n",
      "[rank0]:     module._apply(fn)\n",
      "[rank0]:   File \"/export/home/vgiusepp/miniconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 779, in _apply\n",
      "[rank0]:     module._apply(fn)\n",
      "[rank0]:   File \"/export/home/vgiusepp/miniconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 804, in _apply\n",
      "[rank0]:     param_applied = fn(param)\n",
      "[rank0]:   File \"/export/home/vgiusepp/miniconda3/envs/test/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1159, in convert\n",
      "[rank0]:     return t.to(\n",
      "[rank0]: RuntimeError: CUDA error: out of memory\n",
      "[rank0]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "[rank0]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "[rank0]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "E0505 08:09:42.098000 140322298397312 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 0 (pid: 2936683) of binary: /export/home/vgiusepp/miniconda3/envs/test/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/test/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/test/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/test/lib/python3.10/site-packages/torch/distributed/run.py\", line 879, in main\n",
      "    run(args)\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/test/lib/python3.10/site-packages/torch/distributed/run.py\", line 870, in run\n",
      "    elastic_launch(\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/test/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/test/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 263, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/export/home/vgiusepp/miniconda3/envs/test/lib/python3.10/site-packages/CASBI/distributed_training.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-05-05_08:09:42\n",
      "  host      : compgpu4.iwr.uni-heidelberg.de\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 2936683)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script execution failed with error: 1\n"
     ]
    }
   ],
   "source": [
    "utils.distributed_training(path_train_dataframe=path_train_dataframe, test_and_nll_path=test_and_nll_path, world_size=1, number_of_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
