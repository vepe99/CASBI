{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish prepare the data\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import yaml\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from ili.dataloaders import StaticNumpyLoader\n",
    "from ili.inference import InferenceRunner\n",
    "from ili.validation import ValidationRunner\n",
    "import ili\n",
    "\n",
    "from CASBI.utils.create_dataframe import rescale\n",
    "\n",
    "device='cuda'\n",
    "\n",
    "N_subhalos = 2\n",
    "data = pd.read_parquet('../../../../data/dataframe/dataframe.parquet')\n",
    "data = rescale(data, mean_and_std_path='../../../../data/preprocess/mean_and_std.parquet', scale_observations=True, scale_parameter=True, inverse=True) \n",
    "data =  data.drop(['gas_log10mass', 'a','redshift', 'mean_metallicity', 'std_metallicity','mean_FeMassFrac', 'std_FeMassFrac', 'mean_OMassFrac', 'std_OMassFrac'], axis=1)\n",
    "min_feh, max_feh = min(data['feh']), max(data['feh'])\n",
    "min_ofe, max_ofe = min(data['ofe']), max(data['ofe'])\n",
    "conditions = data[data.columns.difference(['feh', 'ofe', 'Galaxy_name'], sort=False)].drop_duplicates()\n",
    "\n",
    "minimum_theta = [conditions[col].values.min() for col in conditions.columns]   \n",
    "maximum_theta = [conditions[col].values.max() for col in conditions.columns]       \n",
    "minimum_theta = np.array(minimum_theta)\n",
    "maximum_theta = np.array(maximum_theta)\n",
    "def repeat_array(arr, repetitions):\n",
    "    return np.repeat(arr, repetitions)\n",
    "repeat_minimum_theta = repeat_array(minimum_theta, N_subhalos)\n",
    "repeat_maximum_theta = repeat_array(maximum_theta, N_subhalos) \n",
    "\n",
    "prior = ili.utils.Uniform(low=repeat_minimum_theta, high=repeat_maximum_theta, device=device)\n",
    "\n",
    "N_test = 1_000\n",
    "def preprocess_testset(i):\n",
    "    galaxies = set(data['Galaxy_name'].drop_duplicates().sample(N_subhalos, random_state=i))\n",
    "    parameters =  data[data['Galaxy_name'].isin(galaxies)].drop(['feh', 'ofe', 'Galaxy_name'], axis=1).drop_duplicates().values.T.reshape(-1)\n",
    "    galaxy_data = data[data['Galaxy_name'].isin(galaxies)].values\n",
    "    histogram_galaxy, _, _ = np.histogram2d(galaxy_data[:, 0], galaxy_data[:, 1], bins=64, range=[[min_feh, max_feh], [min_ofe, max_ofe]])\n",
    "    sim_data =  np.expand_dims(np.log10(histogram_galaxy + 1e-6 +1), axis=0)\n",
    "    return parameters, sim_data, galaxies\n",
    "\n",
    "# Create a pool of workers\n",
    "with Pool() as pool:\n",
    "    # Map the function to the data\n",
    "    results = pool.map(preprocess_testset, range(N_test))\n",
    "    \n",
    "# Unpack the results\n",
    "theta_test, x_test, galaxies_test = zip(*results)\n",
    "\n",
    "#take the first test set element as x_0 and theta_0    \n",
    "galaxies_0 = galaxies_test[0]\n",
    "data_to_plot_halos = data[data['Galaxy_name'].isin(galaxies_0)].to_parquet('./halos_0.parquet')\n",
    "theta_0 =  theta_test[0]\n",
    "x_0 =  x_test[0]\n",
    "\n",
    "N = 10_000\n",
    "def process_sample(i):\n",
    "    galaxies = data['Galaxy_name'].drop_duplicates().sample(N_subhalos, random_state=i+int(time.time()))\n",
    "    while (any(set(galaxies) == galaxy_in_testset for galaxy_in_testset in galaxies_test)):\n",
    "        print('matched galaxies, try again')\n",
    "        print('galaxies', set(galaxies))\n",
    "        print('test galaxies', galaxies_test)\n",
    "        galaxies = data['Galaxy_name'].drop_duplicates().sample(N_subhalos, random_state=i)\n",
    "    parameters =  data[data['Galaxy_name'].isin(galaxies)].drop(['feh', 'ofe', 'Galaxy_name'], axis=1).drop_duplicates().values.T.reshape(-1)\n",
    "    galaxy_data = data[data['Galaxy_name'].isin(galaxies)].values\n",
    "    histogram_galaxy, _, _ = np.histogram2d(galaxy_data[:, 0], galaxy_data[:, 1], bins=64, range=[[min_feh, max_feh], [min_ofe, max_ofe]])\n",
    "    sim_data =  np.expand_dims(np.log10(histogram_galaxy + 1e-6 +1), axis=0)\n",
    "    return parameters, sim_data\n",
    "\n",
    "# Create a pool of workers\n",
    "with Pool() as pool:\n",
    "    # Map the function to the data\n",
    "    results = pool.map(process_sample, range(N))\n",
    "\n",
    "# Unpack the results\n",
    "theta, x = zip(*results)\n",
    "\n",
    "#save in .npy files, we remove the first element of the test set since it will be stored as x_0 and theta_0\n",
    "np.save('./x_test.npy', x_test[1:])\n",
    "np.save('./theta_test.npy', theta_test[1:])\n",
    "np.save('./x_0.npy', x_0)\n",
    "np.save('./theta_0.npy', theta_0)\n",
    "np.save('./x.npy', x)\n",
    "np.save('./theta.npy', theta)\n",
    "print('finish prepare the data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_channel, output_dim):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, 8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(32 * 8 * 8, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_layers(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc_layers(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, device):\n",
    "    # Suggest values for the hyperparameters\n",
    "    model = trial.suggest_categorical('model', ['maf', 'nsf'])\n",
    "    hidden_features = trial.suggest_categorical('hidden_features', [10, 50, 70, 100])\n",
    "    num_transforms = trial.suggest_categorical('num_transforms', [5, 10, 15, 20])\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 5e-5, 1e-4]) #suggest_loguniform('learning_rate', 1e-5, 1e-4)\n",
    "    output_dim = trial.suggest_categorical('output_dim', [5, 10, 32, 64])\n",
    "        \n",
    "    # reload all simulator examples as a dataloader\n",
    "    all_loader = StaticNumpyLoader.from_config(\"./data.yaml\")\n",
    "    \n",
    "    embedding_net = ConvNet(input_channel=1, output_dim=output_dim)\n",
    "    \n",
    "    nets = [ ili.utils.load_nde_sbi(engine='NPE', model=model, hidden_features=hidden_features, num_transforms=num_transforms, embedding_net=embedding_net)]\n",
    "\n",
    "    train_args = {\n",
    "    'training_batch_size': 1024,\n",
    "    'learning_rate': learning_rate,\n",
    "    'stop_after_epochs': 20}\n",
    "    \n",
    "    runner = InferenceRunner.load(\n",
    "    backend='sbi',\n",
    "    engine='NPE',\n",
    "    prior=prior,\n",
    "    nets=nets,\n",
    "    device=device,\n",
    "    embedding_net=embedding_net,\n",
    "    train_args=train_args,\n",
    "    proposal=None,\n",
    "    out_dir=None,)\n",
    "\n",
    "    # train a model to infer x -> theta. save it as toy/posterior.pkl\n",
    "    # runner = InferenceRunner.from_config(f\"./training.yaml\")\n",
    "    _, summaries = runner(loader=all_loader)\n",
    "    \n",
    "    return summaries[0]['validation_log_probs'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-29 16:10:05,049] Using an existing study with name 'example_study' instead of creating a new one.\n"
     ]
    }
   ],
   "source": [
    "study_name = 'example_study'  # Unique identifier of the study.\n",
    "storage_name = 'sqlite:///example.db'\n",
    "study = optuna.create_study(study_name=study_name, storage=storage_name, load_if_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-134:\n",
      "Process SpawnPoolWorker-130:\n",
      "Process SpawnPoolWorker-129:\n",
      "Process SpawnPoolWorker-131:\n",
      "Process SpawnPoolWorker-132:\n",
      "Process SpawnPoolWorker-135:\n",
      "Process SpawnPoolWorker-133:\n",
      "Traceback (most recent call last):\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'optimize_on_device' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "AttributeError: Can't get attribute 'optimize_on_device' on <module '__main__' (built-in)>\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'optimize_on_device' on <module '__main__' (built-in)>\n",
      "AttributeError: Can't get attribute 'optimize_on_device' on <module '__main__' (built-in)>\n",
      "AttributeError: Can't get attribute 'optimize_on_device' on <module '__main__' (built-in)>\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'optimize_on_device' on <module '__main__' (built-in)>\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'optimize_on_device' on <module '__main__' (built-in)>\n",
      "Process SpawnPoolWorker-136:\n",
      "Traceback (most recent call last):\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/export/home/vgiusepp/miniconda3/envs/fff/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "AttributeError: Can't get attribute 'optimize_on_device' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m num_gpus \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m multiprocessing\u001b[38;5;241m.\u001b[39mPool(num_gpus) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimize_on_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fff/lib/python3.10/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fff/lib/python3.10/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fff/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fff/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/fff/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import multiprocessing\n",
    "\n",
    "def optimize_on_device(device_id):\n",
    "    device = f'cuda:{device_id}'\n",
    "    study.optimize(lambda trial: objective(trial, device), n_trials=100)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    current_start_method = multiprocessing.get_start_method()\n",
    "    if current_start_method is None or current_start_method.lower() != 'spawn':\n",
    "        multiprocessing.set_start_method('spawn', force=True)\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    with multiprocessing.Pool(num_gpus) as p:\n",
    "        p.map(optimize_on_device, range(num_gpus))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
