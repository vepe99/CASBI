\chapter{CASBI}

\section{SBI}
CASBI is a Simulation Based Inference (SBI) package to recover the properties of building blocks of Milky Way like galaxy's halo from observations of the chemical abundance plane. The SBI framework has existed along side the more traditional likelihood based inference methods for quite some years already, and has its root in the Approximate Bayes Computation (Rubin 1984), and it has been used in a variety of fields, from cosmology to particle physics. The main difference between SBI and likelihood based methods, like MCMC, is that the former do not require the likelihood function to be known, but rather rely on a simulator to generate synthetic data \textbf{$\mathbf{x}$} once the input parameters $\boldsymbol{\theta}$ are passed to it, and the inference pipeline is trained based on data-parameters pairs ($\mathbf{x}, \boldsymbol{\theta}$). 

Recent advance of this technique was made possible by the use of machine learning models to emulate conditional probability distributions, a technique know as Neural Density Estimation (NDE) (Papamakarios 2019). The NDE is achieved by training a Normalizing Flow architecture, a generative model that allows to obtain samples from a complex distribution $p(x)$ by constructing a series of \textbf{bijiective} transformations  $f_{\phi_i}^i$ that map $x$ to a latent space $z$ that is distributed as a simple distribution, like a Gaussian. Accordingly to \cite{kingmaGlowGenerativeFlow2018}, in the end the models learns the following schema parameters $\phi_i$:
\begin{equation}
p(x) \sim x \xleftrightarrow{\text{$f_{\phi_1}^1$}} h_1 \xleftrightarrow{\text{$f_{\phi_2}^2$}} h_2 \dots \xleftrightarrow{\text{$f_{\phi_K}^K$}} z \sim \mathcal{N}(z; 0, \mathcal{I}),
\end{equation}
by maximizing the negative log likelihood as loss function and using the change of variable formula as follows:
\begin{equation}
\begin{aligned}
    \text{log} \, p(x) &= \text{log} \, p(z) + \text{log} \left| \text{det} \left( \frac{\partial z}{\partial x} \right) \right| \\
    &= \text{log} \, p(z) + \sum_{i=1}^K \text{log} \left| \text{det} \left( \frac{\partial h_{i}}{\partial h_{i-1}} \right) \right| ,
\end{aligned}
\end{equation}
where the last term is sum of the log determinant of the Jacobian of the transformations $f_{\phi_i}^i$. Once the model is trained it is easy to sample from the distribution $p(x)$ by sampling from the latent space $z$ and applying the inverse transformations $(f_{\phi_1}^1)^{-1} \circ \dots \circ (f_{\phi_K}^K)^{-1}$.
In order to keep the sum of log determinant tractable, the use of \textit{Coupling layers} allows to split the input $x$ along its dimensions and apply a transformation only to a subset of the dimensions, using the other as input for the transformation and keeping it fixed. The subset is than changed at each layers, allowing to have a permutation invariant transformation. The transformations $f^{i}_{\phi_i}$ are usually very simple invertible transformation like a translation and a scaling, or splines functions. 

Following the discussion presented in \cite{hoLtUILIAllinOneFramework2024}, in Bayesian analysis we have the choice to approximate either the Posterior, the Likelihood or the Likelihood ratio, and this choice depend mostly on the problem that one wants to solve. In our case, due to the complexity of the Likelihood distribution of the chemical abundance space, we choose to approximate the Posterior distributions, and so we adopted the Neural Posterior Estimate that can be trained using the negative loglikelihood as loss function:
\begin{equation}
\begin{aligned}
    \mathcal{L}_{NPE}(\boldsymbol{\theta}) &= - \mathbb{E}_{\mathcal{D}_{train}} \text{log} \hat{\mathcal{P}}(\theta_i | x_i) \\ 
    &= - \mathbb{E}_{\mathcal{D}_{train}} \text{log} \left( \frac{p(\theta)}{\tilde{p}(\theta)} q_{\omega}(\theta_i, x_i) \right), 
\end{aligned}
\end{equation}
where our Posterior distribution $ \hat{\mathcal{P}}(\theta_i | x_i)$ is approximated by the product of the ratio of the prior $p(\theta)$ and proposal distribution $\tilde{p}(\theta)$ and the neural conditional distribution $q_{\omega}(\theta_i, x_i)$, parametrized by the parameters $\omega$. 

Many excellent framework for handling SBI analysis are already available, and CASBI is build on top of the \texttt{ltu-ili} python package \cite{hoLtUILIAllinOneFramework2024}. In particular, CASBI analysis were performed relying on the \texttt{sbi} backend \cite{tejero-canteroSbiToolkitSimulationbased2020} to train a \textit{Neural Posterior Estimate} of the parameters' posteriors. The preprocessing of the data is described in Section~\ref{sec:NIHAO}, the details of the training of the NPE is described in Section~\ref{sec:Two step Inference}, and the final package is described in Section~\ref{sec:Python package}.


\section{NIHAO}\label{sec:NIHAO}
The data-parameters pairs are obtained from a set of N-body simulations of Milky Way like galaxies, the \textbf{NIHAO} project \cite{wangNIHAOProjectReproducing2015}. The \textbf{NIHAO} (Numerical Investigation of a Hundred Astrophysical Objects) is a set of 100 cosmological zoom-in hydrodynamical simulations with halos that range from dwarf ($M_{star} \sim 5 \times 10^9 M_\odot$) to Milky Way ($M_{star} \sim 2 \times 10^12 M_\odot$). In order to handle this simulations, in CASBI the preprocessing is done with the use of the functions available in \texttt{pynbody} \cite{pontzenPynbodyNBodySPH2013}. Similarly to \cite{cunninghamReadingCARDsImprint2022} and \cite{deasonUnravellingMassSpectrum2023}, we rely on the assumption that once the accreted object falls into the gravitational potential of the Milky Way like galaxy the star formation rate is halted, so we can treat each of the snapshot in this simulations as a possible building block of galactic halo. In order to create observables we construct 2D histogram \textbf{$\mathbf{x^i}$} of counts of the chemical abundance plane $[O/Fe]$ over $[Fe/H]$ for each of the snapshot available in \textbf{NIHAO}, after filtering in order to have snapshot with a total stellar mass $M_{star} < 6 \times 10^9$, which is the stellar mass of the Large Magellanic Cloud, the largest accreted object by the Milky Way. The actual observable \textbf{$\mathbf{x^j} = \sum_i^{N_{sub}^j} x^j_i$ } used in CASBI is then a super imposition of $N_{sub}$ of these 2D histograms, where the $N_{sub}^j$ is the number of accreted objects present in the galaxy halo that we have used as templates. In CASBI the inference is done on two set of parameters, the $N_{sub}^j$ and the \textbf{$\mathbf{\theta^j}$} = ($M_{star, i}^j, M_{dm, i}^j, \tau_i^j $) with $i=2, ..., N_{sub}$, which are respectively the total stellar mass, the total dark matter mass, and the infall time (in Gyr) of the $i$-th accreted object in the $j$-th sample. 
  

\section{Two step Inference}\label{sec:Two step Inference},

\section{Python package}\label{sec:Python package}