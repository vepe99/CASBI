\chapter{CASBI: Chemical Abundance Simulation Based Inference}

\section{Simulation Based Inference}
CASBI is a Simulation Based Inference (SBI) package to recover the properties of building blocks of Milky Way like galaxy's halo from observations of the chemical abundance plane. The SBI framework has existed along side the more traditional likelihood based inference methods for quite some years already, having its root in the Approximate Bayes Computation \cite{rubinBayesianlyJustifiableRelevant1984}, and it has been used in a variety of fields, from cosmology to particle physics. The main difference between SBI and likelihood based methods, like MCMC, is that the former do not require the likelihood function to be known, but rather rely on a simulator to generate synthetic data \textbf{$\mathbf{x}$} once the input parameters $\boldsymbol{\theta}$ are passed to it, and the inference pipeline is trained based on data-parameters pairs ($\mathbf{x}, \boldsymbol{\theta}$). 

Recent advance of this technique was made possible by the use of machine learning models to emulate conditional probability distributions, a technique know as Neural Density Estimation (NDE) \cite{papamakariosNeuralDensityEstimation2019}. The NDE is achieved by training a Normalizing Flow architecture, a generative model that allows to obtain samples from a complex distribution $p(x)$ by constructing a series of \textbf{bijiective} transformations  $f_{\phi_i}^i$ that map $x$ to a latent space $z$ that is distributed as a simple distribution, like a Gaussian. Accordingly to \cite{kingmaGlowGenerativeFlow2018}, implementing the transformations as Neural Network with parameters $\phi_i$, in the end the models learns the following schema:
\begin{equation}
p(x) \sim x \equiv h_0 \xleftrightarrow{\text{$f_{\phi_1}^1$}} h_1 \xleftrightarrow{\text{$f_{\phi_2}^2$}} h_2 \dots \xleftrightarrow{\text{$f_{\phi_K}^K$}} h_K \equiv z \sim \mathcal{N}(z; 0, \mathcal{I}),
\end{equation}
by maximizing the negative log likelihood as loss function and using the change of variable formula as follows:
\begin{equation}
\begin{aligned}
    \text{log} \, p(x) &= \text{log} \, p(z) + \text{log} \left| \text{det} \left( \frac{\partial z}{\partial x} \right) \right| \\
    &= \text{log} \, p(z) + \sum_{i=1}^K \text{log} \left| \text{det} \left( \frac{\partial h_{i}}{\partial h_{i-1}} \right) \right| \\
    &= \text{log} \, p(z) + \sum_{i=1}^K \text{log} \left| \text{det} \left( \frac{\partial f_{\phi_i}^i(h_{i-1})}{\partial h_{i-1}} \right) \right|,
\label{eq:ChangeOfVariable}
\end{aligned}
\end{equation}
where the last term is the sum of the log determinant of the Jacobian of the transformations $f_{\phi_i}^i$. Once the model is trained it is easy to sample from the distribution $p(x)$ by sampling from the latent space $z$ and applying the inverse transformations $(f_{\phi_1}^1)^{-1} \circ \dots \circ (f_{\phi_K}^K)^{-1}$.
In order to keep the sum of log determinant tractable, the use of \textit{Coupling layers} allows to split the input $x$ along its dimensions and apply a transformation only to a subset of the dimensions, using the other as input for the transformation and keeping it fixed. The subset is than changed at each layers, allowing to have a permutation invariant transformation. The transformations $f^{i}_{\phi_i}$ are usually very simple invertible transformation like a translation and a scaling, or splines functions. The choice of the invertible function can affect the expressivity of the model, defined as the capability of approximate more complex multivariate distribution, at the cost of more parameters, computational time and inference time. 

Following the discussion presented in \cite{hoLtUILIAllinOneFramework2024}, in Bayesian analysis we have the choice to approximate either the Posterior, the Likelihood or the Likelihood ratio, and this choice depend mostly on the problem that one wants to solve. 
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{./figure/sbi_methods.jpg}
    \caption{Different approaches to Simulation Based Inference, from \cite{FrontierSimulationbasedInference}.}
    \label{fig:sbi_approaches}
\end{figure}

In our case, due to the complexity of the Likelihood distribution of the chemical abundance space, we choose to approximate the Posterior distributions, and so we adopted the Neural Posterior Estimate (method \textbf{F} in Figure~\ref{fig:sbi_approaches}) that can be trained using the negative loglikelihood as loss function:
\begin{equation}
\begin{aligned}
    \mathcal{L}_{NPE}(\boldsymbol{\theta}) &= - \mathbb{E}_{\mathcal{D}_{train}} \text{log} \hat{\mathcal{P}}(\theta_i | x_i) \\ 
    &= - \mathbb{E}_{\mathcal{D}_{train}} \text{log} \left( \frac{p(\theta)}{\tilde{p}(\theta)} q_{\omega}(\theta_i, x_i) \right), 
\end{aligned}
\end{equation}
where our Posterior distribution $ \hat{\mathcal{P}}(\theta_i | x_i)$ is approximated by the product of the ratio of the prior $p(\theta)$ and proposal distribution $\tilde{p}(\theta)$ and the neural conditional distribution $q_{\omega}(\theta_i, x_i)$, parametrized by the parameters $\omega$. 

Many excellent framework for handling SBI analysis are already available, and CASBI is build on top of the \texttt{ltu-ili} python package \cite{hoLtUILIAllinOneFramework2024}. In particular, CASBI analysis were performed relying on the \texttt{sbi} backend \cite{tejero-canteroSbiToolkitSimulationbased2020} to train a \textit{Neural Posterior Estimate}\footnote{The \texttt{sbi} backed implement NPE using \textbf{\texttt{nflows}} \cite{durkanNflowsNormalizingFlows2020}} of the parameters' posteriors. The preprocessing of the data is described in Section~\ref{sec:NIHAO}, the details of the training of the NPE is described in Section~\ref{sec:Two step Inference}.


\section{Simulator}\label{sec:NIHAO}
The data-parameters pairs $(\mathbf{x}, \boldsymbol{\theta})$ needed to train the NPE are obtained from the Numerical Investigation of a Hundred Astrophysical Objects (\textbf{NIHAO}) project \cite{wangNIHAOProjectReproducing2015}. The \textbf{NIHAO} is a set of 100 cosmological zoom-in hydrodynamical simulations with halos that range from dwarf ($M_{star} \sim 5 \times 10^9 M_\odot$) to Milky Way like ($M_{star} \sim 2 \times 10^{12} M_\odot$). In order to handle these simulations, in CASBI the preprocessing is done with the use of the functions available in \texttt{pynbody} \cite{pontzenPynbodyNBodySPH2013}. In Fig. \ref{fig:NIHAO} we show face on samples of galaxies in the \textbf{NIHAO} simulations set.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{./figure/NIHAO.jpeg}
    \caption{Face on \textbf{NIHAO} galaxies from \cite{wangNIHAOProjectReproducing2015}.}
    \label{fig:NIHAO}
\end{figure}

Similarly to \cite{cunninghamReadingCARDsImprint2022} and \cite{deasonUnravellingMassSpectrum2023}, we rely on the assumption that once the accreted object falls into the gravitational potential of the Milky Way like galaxy its star formation rate is halted, so we can treat each of the snapshot in this simulations as a possible building block of galactic halo. 

The construction of the observables is done in by aggregating multiple subhalo into a single stellar halo. In order to create subhalo we construct 2D histogram, referred to as \textbf{$\mathbf{x^i}$}, by binning the chemical abundance plane [$[O/Fe]$, $[Fe/H]$]\footnote{They are respectively proxy for $\alpha$ elements abundance and metallcity} for each of the snapshot available in \textbf{NIHAO}. We have also filter the galaxies to have object with a total stellar mass lower than the stellar mass of the Large Magellanic Cloud ($M_{star} < 6 \times 10^9 M_\odot$), the largest accreted object by the Milky Way. The 2D histogram have $64 \times 64$ pixels, and minimum and maximum values set after filtering all the stars that were outside the 0.01 percentile in either metallicity or $\alpha$ element abundance. Each of the $x_i$ is uniquely identifiable trough the \texttt{Galaxy\_name} attribute. The set of all possible subhalos is defined as 'Template Library'. The actual stellar halo observable \textbf{$\mathbf{x^j} = \sum_i^{N_{sub}^j} x^j_i$ } used in CASBI is then a super imposition of $N_{sub}$ of these 2D histograms, where the $N_{sub}^j$ is the number of accreted objects present in the $j$-th galaxy halo. The actual choice of how to sample from the template library created from the \textbf{NIHAO} simulations can be adapted, we tested to randomly sample in \ref{sec:Two step Inference} and to use a more physically informed approach by using a luminosity function and a total stellar mass budget in Section \ref{sec:Realistic halo and 1 step Inference}. 

% In CASBI the inference is done on two set of parameters, the $N_{sub}^j$ and the \textbf{$\mathbf{\theta^j}$} = ($M_{star, i}^j, M_{dm, i}^j, \tau_i^j $) with $i=2, ..., N_{sub}$, which are respectively the total stellar mass, the total dark matter mass, and the infall time (in Gyr) of the $i$-th accreted object in the $j$-th sample. The inference pipeline is further described in  the following section.
  
The goal of CASBI is to being able to recover $\mathbf{\theta^i}$ for each of the subhalos in the galactic halo from the observable \textbf{$\mathbf{x^j}$}$=\sum_i x^j_i$, and gaining insight on how many subhalos there are. Among all the possible parameters available from the simulations, we have decided to limit ourself to stellar mass $M_{star}$ and age of the galaxy $\tau$, also called infall time due to their equivalence in the assumption of quenched star formation after accretion.  

\section{Free Form Flow as a surrogate simulator} \label{sec:SurrogateSimulator}
If it is possible to generate new data pairs at inference time, by sampling the prior and passing the samples to the simulator, and sequentially repeat the inference it is possible to achieve better accuracy, as it shown empirically in Fig. \ref{fig:sequential_SBI}, at the cost of losing the \textbf{ammortize} property.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{./figure/sequential_SBI.png}
    \caption{Different SBI method accuracy over 10 independent test with similar simulation budget. The error is defined in terms of the Classifier 2-sample Test (C2ST), with 0.5 being the optimal score. Figure credit \cite{hoLtUILIAllinOneFramework2024}. }
    \label{fig:sequential_SBI}
\end{figure}

Inspired by this result, we have decided to explorer the possibility of implement Sequential Neural Posterior Estimate (SNPE) in CASBI. The use of the simulator at inference time is not usually possible for cosmological application, in fact running those simulations is both computationally and time consuming. We have decided to implement a surrogate simulator that aims to learn how to sample new observation $x$ from the Likelihood probability $p(x|\theta)$. The surrogate simulator that we adopted was Free Form Flow (FFF) \cite{draxlerFreeformFlowsMake2024}, a Normalizing Flow architecture that relaxes the need for invertible transformations and only requires dimensionality preservation at each step. After trying to use the more common Neural Spline Flow (NSF), like in \cite{wolfGalacticFlowLearningGeneralized2023}, we decided to adopt FFF due to higher fidelity in the generation of new observation $x$, using the $D$-statistic of the two dimensional Kolmogorov-Smirnov Test as fidelity metric \cite{lopesTwodimensionalKolmogorovSmirnovTest}. The architecture of the FFF is composed of a encoder and a decoder that take the role respectively of the forward transformation $f_\theta$ to the normally distributed latent space, and an approximation of the inverse transformation $g_\phi$ that map the latent space to the observation space. The major innovation in Free Form Flow that are needed to understand the choice and flexibility of this architecture are:

\begin{itemize}
\item Gradient Trick: since the most computationally expensive part of the loss function of normalizing flow is calculating the Jacobian of the transformation, the authors propose to  estimate its gradient using a pairs of vector-Jacobian and Jacobian-vector products easily available in standard automatic differentiation libraries. The gradient trick is implement in the pipeline by rewriting the maximum likelihood loss derived from equation \ref{eq:ChangeOfVariable} as:
\begin{equation}
        \mathcal{L}_{ML}^{f^{-1}} = \mathbb{E}_{x, v}[-\text{log}p(f_\theta (x)) - v^T J_\theta \text{SG}(J^{-1}v)], 
\label{eq:MLloss}
\end{equation}
where $v$ is a random vector with unit variance with the same dimensions as $x$, and SG is the Stop Gradient operation. This loss enable to train normalizing flow architecture with a tractable inverse function whose Jacobian determinant is not easily accessible.

\item Inverse Approximation: classical normalizing flow architecture require the access the analytic inverse of the transformations $f_\theta^{-1}$, either by constructing Invertible Neural Network or defining the flow with a differential equation with a known reverse time process. The authors propose to approximate the inverse with a learned inverse $g_\phi \approx f_\theta^{-1}$. The loss function is then modified to learn this approximation with the following contribution, called reconstruction loss: 

\begin{equation}
    \mathcal{L}_{R} = \frac{1}{2}\mathbb{E}_{x}[|| x - g_\phi(f^{-1}_\theta (x)) ||^2].
\label{eq:Rloss}
\end{equation}
This part allows to remove the architectural constrains from $f_\theta$ and $g_\phi$ except for preserving the dimensions.
\end{itemize}

Combining both contributions from equations \ref{eq:MLloss} and \ref{eq:Rloss} leads to the following loss function:

\begin{equation}
    \mathcal{L}_{FFF}^g = \mathcal{L}_{ML}^{g} + \beta \mathcal{L}_{R},
\label{eq:FFFloss}
\end{equation}
where the $\mathcal{L}_{ML}^{g}$ is used in place of the $\mathcal{L}_{ML}^{f^{-1}}$ with the justification that they have the same critical points and the $\beta$ is a trade off hyperparameter. For a more in depth explanation and a mathematical foundation of FFF architecture we refer to the original paper \cite{draxlerFreeformFlowsMake2024}. 

In CASBI we have choose to use a Skip Connection Multi Layer Perceptron (SC-MLP) as both encoder and decoder, and we have followed the suggestions in Appendix B.1 of \cite{draxlerFreeformFlowsMake2024} to make this architecture conditional by concatenating to each layer the parameters $\theta$ sampled from our prior distributions.  
Even though the FFF architecture has good interpolation capability, returning average $D$-statics values lower than 0.3 when reconstructing the test set, the problem of sampling independent parameters values from the prior distributions make the net to extrapolate in regions of the conditional space where no data were shown, degrading the generate abundance halo. For example it could happen that a very massive $M_{star}$ is sampled together with a very low infall time $\tau$, which is a combination of parameters that is not physical and hence was not shown during training, so the generated halo is not realistic, namely it has a very high $D$-statistic value. Do to general poorer performance of SNPE using the FFF as a surrogate simulator, we have decided to not include this architecture in the analysis. Future work could incorporate some level of correlation in the prior distribution of the parameters, since currently the \texttt{ltu-ili} package allows only for independent priors.



\section{Two step Inference}\label{sec:Two step Inference}
The objective of the inference is not trivial, since in order to recover the parameters of the building blocks of the Milky Way like galaxy we need to fix the dimensionality of the priors. This is equivalent to have complete knowledge on the number of substructure that are present in the galactic halo. In the case of not fully phase mixed structure, the dynamical information could be use to help to disentangle this structure, and also to separate them from the host halo background. In CASBI we do not leverage on this information because it would require to construct stellar halo that have aggregated objects that are not dynamically biased. We leave this integration for future work. We decided to tackle this problem in the case of fully mixed remnants separating the inference in two steps, in the first we inferr the number of subhalos and in the second the parameters of each of the subhalos:
\begin{enumerate}
    \item \textbf{Inference of the number of substructure}: In this step we train a NPE to recover the posterior distribution of the number of substructure $N_{sub}$, by using the observable \textbf{$\mathbf{x^j}$}. The prior for the parameter is assumed to be uniform between 2 and 100. This boundaries were selected in accordance to the order of magnitude of substructures found in \cite{deasonUnravellingMassSpectrum2023}. For each of the possible $N_{sub}$ we extract 1000  $x^j = \sum_{i=1}^{N_{sub}} x_i^j$ from the \textbf{NIHAO} simulations, in order to have a total of almost $ 10^5$ SBI training couples $(N_{sub}^j, x^j)$, with 20 \% used as validation, and we use the same process to generate almost $10^4$ test set samples, making sure that the same combinations of \texttt{Galaxy\_name} attribute weren't shown in training and test. The training of the NPE is done using the \texttt{sbi} backend, using 4 \texttt{nsf} (neural spline flow) with 10 layers and 100 neurons each. In order to take full advantage of the image-like structure of the data, we adopt as embedding network a Convolutional Neural Network (CNN) to reduce the dimensionality of the input of the NPE from $64 \times 64$ to 128. The CNN had 3 convolutional layers with 8, 16 and 32 filter, 3 maxplooling layers and 3 fully connected layers with 512, 256, 128 neurons.
    In this step we have not impose that the $N_{sub}$ must be a discrete variable, and we have decide to just truncate the inferred value to the closet value. To the knowledge of the author no SBI framework has implemented a way of dealing with the inference of discrete random variables, so we leave a more precise implementation as a future work. We propose instead another method to obtain the number of substructure, by casting this inference as a classification problem. We use a SkipConnection CNN \footnote{The architecture is the same as the embedding network described before with the addition of the Skip Connection layer in the fully connected layer, where the output of the previous layer gets added to the output before being passed trough the activation function, alleviating the vanishing gradient problem and allowing for better accuracy.}, considering the number of substructure as the label to assign to each $x^j$.

    \item \textbf{Inference of $\mathbf{\theta^j}$}: Once we have the estimate $\tilde{N}_{sub}$, whether using dynamical information, the inference pipeline or the classification method, we can proceed to the inference of the parameters $\mathbf{\theta^j_i} $. The prior for the parameters are assumed to be uniform between the minimum and maximum values available for the galaxies that we have filtered from the \textbf{NIHAO} simulations. We extract $10^5$ random samples of $\tilde{N}_{sub}$ snapshots from the \textbf{NIHAO} simulations, and we construct the observable 
    couples $(x^j, (\theta_1^j, \dots, \theta_{\tilde{N}_{sub}}^j))$, with 20 \% used as validation. We repeat the same process to generate $10^3$ test set samples, making sure that the same combinations of \texttt{Galaxy\_name} attribute weren't shown in training and test to perform calibration of the inference model. The training of the NPE is done using the \texttt{sbi} backend, using 4 \texttt{nsf} (neural spline flow) with 10 layers and 100 neurons each. Once again we use the same CNN architecture of the previous step as embedding for our observation $x^j$.
\end{enumerate}
Even though highly modular, this two step inference has some limitations: the accuracy and calibration of the second step are heavily depend on the ability of the first step to recover the number of sunhalos and hence to constrain the dimensionality of the prior for the second step. We expected the pipeline to be able to recover most of the information from the most massive subhalos, due to the degeneracy in the abundance plane of the less massive and components and the more distinct feature of the more massive one. The second problem is linked to the linear scaling of the parameter dimension as a function of the number of subhalos. In a realistic case we expect to have order of $\approx$100 subhalos, resulting in a parameter space of dimensionality 2*100 = 200. In order to reduce the impact of these two problems we rethink the inference pipeline, presented in the next section.  

% The analysis with this two step inference did not satisfy the author, because the inference on the number of subhalos, or the classification, was not accurate enough to be generate a good foundation for the prior distribution of the second step. The inference pipeline for the parameters $\mathbf{\theta^j}$ did not converge using the fast \texttt{direct sampling}. This could happen because the Normalizing flow architecture do not constrain the support of the posterior, and the inference could get stack because the sampling happens in region outside the boundaries of the prior distribution. In order to overcome this limitation one might decide to switch to a traditional MCMC sampler, but due to the time consuming nature of this method we decided to rethink the inference pipeline, presented in the next subsection.    

\section{Realistic halo and 1 step Inference}\label{sec:Realistic halo and 1 step Inference}


\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{./figure/CASBI_complete.png}
    \caption{CASBI pipeline. In our analysis we have fixed the \textbf{Template Library} to be a subsample of the \textbf{NIHAO} simulations, as described in Section \ref{sec:NIHAO}, but it can be swapped with user-choice simulations. The choice of the Template Library incorporate all the assumption that we make on the chemical enrichment history of Galaxies, the dynamical effects that accreted objects undergo, and the cosmology, making this part the principal cause of possible mispecification. During \textbf{sampling} we sample non-repeated subhalos aiming to reproduce a \textbf{Luminosity function} $N(<L)$ that can be set by the user, for our analysis we have taken it from \cite{koposovLuminosityFunctionMilky2008}.  Moreover a \textbf{stellar mass budget} can be set to generate realistic stellar halo without the need of fixing the number of subhalos (even though a maximum number of subhalos is set in our analysis for computational reasons). In our analysis we fixed the stellar mass budget of the halo accordingly to \cite{deasonTotalStellarHalo2019}, but it can be set by the user accordingly to the estimated total stellar mass available in the observation that we want to infer on. The \textbf{SBI} pipeline can incorporate a \textbf{Surrogate Simulator} to perform the sequential version of NPE. In our analysis we do not adopt this component as described in Section \ref{sec:SurrogateSimulator}. The \textbf{Observational Realism} encapsulate all that concerns bridging the gap between simulation and observation, i.e. uncertainties of spectroscopic surveys, selection functions, etc. In our analysis we haven't inject this component into the pipeline because we aimed to understand the limitations of this technique when no observational contamination is involved.}
    \label{fig:CASBI_complete}
\end{figure}

In order to avoid the need for a two step inference and still retaining the possibility to access to the information on how many subhalos populate a given abundance plane, we have decided to condition the SBI model to retrieve the $i$-most massive subhalo of the $j$-th stellar halo. In this way the NPE is trained on $(x, \theta) = (i, x^j, \theta_i^j)$ pairs, where $x^j = \sum_i x_i^j$ and the $x_i^j$ are ordered accordingly to their stellar mass $M_{star}$. In this way the $j$-th stellar halo abundance plane is shown as many times as the number of subhalos present in it, and in order to guide the model into inferring the right parameters $\theta_i^j$ the embedding is conditioned on the integer $i$-th by concatenating it to each input of the fully connected layers of the CNN used to embed the observations, and it is concatenate also before passing the embedded information to the Normalizing Flow. This conditioning can be appreciated also in the lower part of Fig. \ref{fig:CASBI_complete}, in which the CASBI SBI pipeline shows the integer $i$ as a red node concatenated to each layer of the embedding network. 
            
In order to create more realistic mock galaxy halo we have decided to adopt a sampling scheme for the subhalos that is based on the luminosity function described in \cite{koposovLuminosityFunctionMilky2008}. The luminosity function described the subhalo distribution in a range of luminosities that spans from $M_V = -2$ all the way to the luminosity of the Large Magellanic cloud:
\begin{equation}
    \frac{dN}{d M_V} = 10 \times 10^{0.1(M_V + 5)} 
\end{equation}
we can then manipulate this equation to express it as a function of the Luminosity $L$:
\begin{equation}
\begin{split}
    \frac{dN}{dL} &= \frac{dN}{d M_V} \times \frac{dM_V}{dL} 
    = 10^{0.1(M_{V, \odot} - 2.5\text{log}_{10} (L) +5) +1} \times (-2.5 L^{-1}) \sim L^{-1.25}\\
\end{split}
\end{equation}

which in the end can be integrated to obtain the number of subhalos with luminosity lower the $L$ that we are going to adopt for sampling stellar halo:
\begin{equation}
    N(<L) = K \times L^{1+\alpha},
\label{eq:CumLuminosity}
\end{equation}
where K represent a constant and $\alpha=-1.25$ is the single power law exponent obtained by \cite{koposovLuminosityFunctionMilky2008}. Other work based not only on SDSS observations like \cite{koposovLuminosityFunctionMilky2008} but also on $\Lambda$CDM $N$-body simulation set $\alpha = -1.9 \pm 0.2$ (\cite{tollerudHundredsMilkyWay2008}). We fix this value to -1.25 and we leave the analysis of the impact of this choice as a future work.
Assuming $L_\odot = M_\odot$, we normalize equation \ref{eq:CumLuminosity} after setting the support to be the interval of masses that we have available in our catalogue of NIHAO simulations ($10^5 M_\odot < M_{star} < M_{star}^{halo}$) and we sample from this distributions using an inverse scheme, where $ M_{star}^{halo}$ is the mass budget for our mock halo of $M=1.4 \pm 0.2 \times 10^9 M_\odot$ based on \cite{deasonTotalStellarHalo2019}. After obtaining the analytic samples we take the first and second Nearest Neighbors (NN) that are within a 10\% of the analytic sampled mass as subhalo for our mock halos and we reduce the total mass budget by the mass of the NN that we have used. The choice of the mass budget can be adapted and comes from observation that do not take into account Large and Small Magellanic Cloud, but it can be customized or even set into a range of stellar budget mass at each generation of a mock stellar halo. During this iterative procedure we make sure to sample non repeated subhalo within the same mock halo and we avoid repetitions of the same combinations of subhalos between mock subhalos both within training and test set and across these two sets.

In Fig. \ref{fig:CASBI_complete} we show the CASBI pipeline. The modularity of the SBI technique is fully integrated, allowing to change all the components of this pipeline. The Template library can be set to be a different suite of simulated galaxies (e.g. \cite{pillepichMilkyWayAndromeda2023}), the sampling scheme can incorporate different luminosity function and stellar halo budget, the NPE and embedding network architecture and hyperparameter can be modified to allow for higher accuracy and posterior coverage thanks to the \texttt{optuna} grid search implementation, and surrogate models (Free Form Flow FFF \cite{draxlerFreeformFlowsMake2024}, GRUMPY \cite{kravtsovGRUMPYSimpleFramework2022}) can be implemented to allow for the Sequential version of the NPE.      


\section{Calibration}
This section is highly inspired by \cite{hoLtUILIAllinOneFramework2024}. In posterior estimation we aim to maximize the constraining of $\theta$ given the observable $x_0$ and whether the uncertainties are calibrated to our training data. These criteria are naturally adversarial and the this problem can be interpreted as another instance of the bias-variance trade off. 
It is possible to confront various NPE accuracy by comparing the cumulative posterior value of the test set, $\hat{\mathcal{P}}(D_{test})=\prod_i^{N_{test}} \hat{\mathcal{P}}(\theta_i|x_i)$, since a larger posterior value concentrate more probability mass around the true value, which translate directly to a higher constraining power. Moreover it is possible to gauge the overall constrain power against a \textit{ground truth} posterior, namely long-run MCMC output. A classical metric to confront posterior samples is the C2ST, which is defined as the accuracy of a classifier to distinguish between true and infered posterior samples, with a C2ST value of 0.5 implying that the two sampled distributions are the same. 

The calibration of the model uncertainties can be obtain using the Probability Integral Transformation (PIT), defined as the cumulative density function of our posterior given $x_0$:

\begin{equation}
    \text{PIT}(\theta|x_0) = \int_{-\infty}^{\theta} \hat{\mathcal{P}}(\theta|x_0) d\theta. 
\end{equation}

Due to the poor scaling of the PIT to higher dimension, it is better to construct and estimate PIT value as:

\begin{equation}
    \text{PIT}(\theta|x_0) = \mathbb{E}_{\hat{\theta}\sim \hat{\mathcal{P}}(\theta|x)}[\Theta (\hat{\theta} - \theta)],
\label{eq:estimated_PIT}
\end{equation}

where $\Theta$ is the Heaviside step function. The PIT counts the number of time the posterior samples $\hat{\theta}$ fall below the true parameter value $\theta$.  If we match the true posterior everywhere we expect the PIT value to be distributed uniformly in range [0, 1] for each of the test set samples. Usually the PIT distribution is studied using percentile-percentile (P-P) plots, comparing the CDF of the PIT value to the CDF of a uniform distribution. This tool can be used to constrain over /\ sunder dispersions. In Fig. \ref{fig:calibration} we show an example of uncalibrated posteriors approximation and the corresponding P-P plot \footnote{The plot is not accurate, it is created to give an idea of the general relation between the posterior behavior and how it is reflected in the P-P plot.}. Since as the dimensionality of $\theta$ increase the proper coverage requires exponentially more samples, we decide to rely on the PIT of each component $\theta_i$ of the marginal posterior, and we expect that the value
\begin{equation}
    \text{PIT}(\theta_i|x_0) = \mathbb{E}_{\hat{\theta}\sim \hat{\mathcal{P}}(\theta_i|x)}[\Theta (\hat{\theta_i} - \theta_i)],  
\label{eq:estimated_marginal_PIT}
\end{equation}
has the same properties as the PIT obtain in equation \ref{eq:estimated_PIT} if the model is globally consistent on the test set.

Lastly, as an approximation to check multivariate posterior coverage, we use the Test of Accuracy with Random Points (TARP) \cite{lemosSamplingBasedAccuracyTesting}. TARP constructs, in the limit of sufficient samples, an estimate of posterior coverage which is guaranteed to converge to the true posterior coverage. 
All of the previously  described posterior coverage method are already integrated in the \texttt{ltu-ili} package. 
\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth]{./figure/calibration.png}
    \caption{Examples of SBI uncalibrated posterior. Left panel: over (red) and under (green) posterior obtained from the same data $\theta$, for which the true posterior (blue) is shown. Right panel: P-P plot for the two uncalibrated posterior. The goal of the calibration is to obtain a distribution that is underconfident and as close as possible to the diagonal. This figure is inspired by \cite{falkiewiczCalibratingNeuralSimulationBased}. }
    \label{fig:calibration}
\end{figure}

