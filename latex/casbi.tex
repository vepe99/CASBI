\chapter{CASBI}

\section{SBI}
CASBI is a Simulation Based Inference (SBI) package to recover the properties of building blocks of Milky Way like galaxy's halo from observations of the chemical abundance plane. The SBI framework has existed along side the more traditional likelihood based inference methods for quite some years already, and has its root in the Approximate Bayes Computation \cite{rubinBayesianlyJustifiableRelevant1984}, and it has been used in a variety of fields, from cosmology to particle physics. The main difference between SBI and likelihood based methods, like MCMC, is that the former do not require the likelihood function to be known, but rather rely on a simulator to generate synthetic data \textbf{$\mathbf{x}$} once the input parameters $\boldsymbol{\theta}$ are passed to it, and the inference pipeline is trained based on data-parameters pairs ($\mathbf{x}, \boldsymbol{\theta}$). 

Recent advance of this technique was made possible by the use of machine learning models to emulate conditional probability distributions, a technique know as Neural Density Estimation (NDE) \cite{papamakariosNeuralDensityEstimation2019}. The NDE is achieved by training a Normalizing Flow architecture, a generative model that allows to obtain samples from a complex distribution $p(x)$ by constructing a series of \textbf{bijiective} transformations  $f_{\phi_i}^i$ that map $x$ to a latent space $z$ that is distributed as a simple distribution, like a Gaussian. Accordingly to \cite{kingmaGlowGenerativeFlow2018}, in the end the models learns the following schema parameters $\phi_i$:
\begin{equation}
p(x) \sim x \xleftrightarrow{\text{$f_{\phi_1}^1$}} h_1 \xleftrightarrow{\text{$f_{\phi_2}^2$}} h_2 \dots \xleftrightarrow{\text{$f_{\phi_K}^K$}} z \sim \mathcal{N}(z; 0, \mathcal{I}),
\end{equation}
by maximizing the negative log likelihood as loss function and using the change of variable formula as follows:
\begin{equation}
\begin{aligned}
    \text{log} \, p(x) &= \text{log} \, p(z) + \text{log} \left| \text{det} \left( \frac{\partial z}{\partial x} \right) \right| \\
    &= \text{log} \, p(z) + \sum_{i=1}^K \text{log} \left| \text{det} \left( \frac{\partial h_{i}}{\partial h_{i-1}} \right) \right| ,
\end{aligned}
\end{equation}
where the last term is sum of the log determinant of the Jacobian of the transformations $f_{\phi_i}^i$. Once the model is trained it is easy to sample from the distribution $p(x)$ by sampling from the latent space $z$ and applying the inverse transformations $(f_{\phi_1}^1)^{-1} \circ \dots \circ (f_{\phi_K}^K)^{-1}$.
In order to keep the sum of log determinant tractable, the use of \textit{Coupling layers} allows to split the input $x$ along its dimensions and apply a transformation only to a subset of the dimensions, using the other as input for the transformation and keeping it fixed. The subset is than changed at each layers, allowing to have a permutation invariant transformation. The transformations $f^{i}_{\phi_i}$ are usually very simple invertible transformation like a translation and a scaling, or splines functions. 

Following the discussion presented in \cite{hoLtUILIAllinOneFramework2024}, in Bayesian analysis we have the choice to approximate either the Posterior, the Likelihood or the Likelihood ratio, and this choice depend mostly on the problem that one wants to solve. In our case, due to the complexity of the Likelihood distribution of the chemical abundance space, we choose to approximate the Posterior distributions, and so we adopted the Neural Posterior Estimate that can be trained using the negative loglikelihood as loss function:
\begin{equation}
\begin{aligned}
    \mathcal{L}_{NPE}(\boldsymbol{\theta}) &= - \mathbb{E}_{\mathcal{D}_{train}} \text{log} \hat{\mathcal{P}}(\theta_i | x_i) \\ 
    &= - \mathbb{E}_{\mathcal{D}_{train}} \text{log} \left( \frac{p(\theta)}{\tilde{p}(\theta)} q_{\omega}(\theta_i, x_i) \right), 
\end{aligned}
\end{equation}
where our Posterior distribution $ \hat{\mathcal{P}}(\theta_i | x_i)$ is approximated by the product of the ratio of the prior $p(\theta)$ and proposal distribution $\tilde{p}(\theta)$ and the neural conditional distribution $q_{\omega}(\theta_i, x_i)$, parametrized by the parameters $\omega$. 

Many excellent framework for handling SBI analysis are already available, and CASBI is build on top of the \texttt{ltu-ili} python package \cite{hoLtUILIAllinOneFramework2024}. In particular, CASBI analysis were performed relying on the \texttt{sbi} backend \cite{tejero-canteroSbiToolkitSimulationbased2020} to train a \textit{Neural Posterior Estimate}\footnote{The \texttt{sbi} backed implement NPE using \textbf{\texttt{nflows}} \cite{durkanNflowsNormalizingFlows2020}} of the parameters' posteriors. The preprocessing of the data is described in Section~\ref{sec:NIHAO}, the details of the training of the NPE is described in Section~\ref{sec:Two step Inference}, and the final package is described in Section~\ref{sec:Python package}.


\section{NIHAO}\label{sec:NIHAO}
The data-parameters pairs are obtained from a set of N-body simulations of Milky Way like galaxies, the \textbf{NIHAO} project \cite{wangNIHAOProjectReproducing2015}. The \textbf{NIHAO} (Numerical Investigation of a Hundred Astrophysical Objects) is a set of 100 cosmological zoom-in hydrodynamical simulations with halos that range from dwarf ($M_{star} \sim 5 \times 10^9 M_\odot$) to Milky Way ($M_{star} \sim 2 \times 10^12 M_\odot$). In order to handle these simulations, in CASBI the preprocessing is done with the use of the functions available in \texttt{pynbody} \cite{pontzenPynbodyNBodySPH2013}. Similarly to \cite{cunninghamReadingCARDsImprint2022} and \cite{deasonUnravellingMassSpectrum2023}, we rely on the assumption that once the accreted object falls into the gravitational potential of the Milky Way like galaxy its star formation rate is halted, so we can treat each of the snapshot in this simulations as a possible building block of galactic halo. In order to create observables we construct 2D histogram, referred to as \textbf{$\mathbf{x^i}$}, of counts of the chemical abundance plane $[O/Fe]$ over $[Fe/H]$ ($\alpha$ element abundance over metallicity) for each of the snapshot available in \textbf{NIHAO}, after filtering in order to have galaxies with a total stellar mass $M_{star} < 6 \times 10^9 M_\odot$, which is the stellar mass of the Large Magellanic Cloud, the largest accreted object by the Milky Way. These 2D histogram have $64 \times 64$ pixels, they have minimum and maximum values set after filtering all the stars that were outside the 0.01 percentile in either metallicity or $\alpha$ element abundance, and they are linked to galaxy snapshot trough the \texttt{Galaxy\_name} attribute. The actual observable \textbf{$\mathbf{x^j} = \sum_i^{N_{sub}^j} x^j_i$ } used in CASBI is then a super imposition of $N_{sub}$ of these 2D histograms, where the $N_{sub}^j$ is the number of accreted objects present in the $j$-th galaxy halo. In CASBI the inference is done on two set of parameters, the $N_{sub}^j$ and the \textbf{$\mathbf{\theta^j}$} = ($M_{star, i}^j, M_{dm, i}^j, \tau_i^j $) with $i=2, ..., N_{sub}$, which are respectively the total stellar mass, the total dark matter mass, and the infall time (in Gyr) of the $i$-th accreted object in the $j$-th sample. The inference pipeline is further described in  the following section.
  
\section{Two step Inference}\label{sec:Two step Inference}
The objective of the inference is not trivial, because in order to recover the parameters of the building blocks of the Milky Way like galaxy we need to know how many parameters we should look for in order to fix the dimensionality of the priors. This is equivalent to have perfect knowledge on the number of substructure that are present in the galactic halo. In the case of not fully phase mixed structure, the dynamical information could be use to help to disentangle this structure, and also to separate them from the host halo background. In CASBI we do not leverage on this information because it would be hard to construct unbiased couples of observables-parameters, and we leave this task for future work. Instead we separate the inference in two steps:
\begin{enumerate}
    \item \textbf{Inference of the number of substructure}: In this step we train a NPE to recover the posterior distribution of the number of substructure $N_{sub}$, by using the observable \textbf{$\mathbf{x^j}$}. The prior for the parameter is assumed to be uniform between 2 and 100. The boundaries were selected in accordance to the order of magnitude of substructures found in \cite{deasonUnravellingMassSpectrum2023}. For each of the possible $N_{sub}$ we extract 1000 random samples of $N_{sub}$ snapshots from the \textbf{NIHAO} simulations, and we construct the observable \textbf{$\mathbf{x^j}$} as described in Section~\ref{sec:NIHAO}, so in total we have $1000 \times 100 = 10^5$ training samples, with 20 \% used as validation, and we use the same process to generate $100 \times 100 = 10^4$ test set samples, making sure that the same combinations of \texttt{Galaxy\_name} attribute weren't shown in training and test. The training of the NPE is done using the \texttt{sbi} backend, using 4 \texttt{nsf} (neural spline flow) with 10 layers and 100 neurons each. In order to take full advantage of the image-like structure of the data, we adopt a CNN embedding network to reduce the dimensionality of the input of the NPE from $64 \times 64$ to 128. The learning rate was set to ..., the batch size was set to .... The hyperparameter were tuned using the \texttt{optuna} package by maximizing the validation log likelihood. In this step we have not impose the constrain that the $N_{sub}$ must be a discrete variable, and we have decide to approximate the sample estimate to an integer. To the knowledge of the author no SBI framework has implemented a way of dealing with the inference of discrete random variables, so we leave a more precise implementation as a future work. We propose instead another method to obtain the number of substructure, by casting this inference as a classification problem. We use a SkipConnection CNN, considering the number of substructure as the label to assign to each $x^j$. The performance for both method are reported in Section \ref{sec:Analysis}.
    \item \textbf{Inference of $\mathbf{\theta^j}$}: Once we have the estimate $\tilde{N}_{sub}$, whether using dynamical information, or the inference pipeline or the classification method, we can proceed to the inference of the parameters $\mathbf{\theta^j}$. The prior for the parameters are assumed to be uniform between the minimum and maximum values available for the galaxies that we have filtered from the NIHAO simulations. We extract $10^5$ random samples of $N_{sub}$ snapshots from the \textbf{NIHAO} simulations, and we construct the observable \textbf{$\mathbf{x^j}$} as described in Section~\ref{sec:NIHAO}, with 20 \% used as validation and the rest as training. We repeat the same process to generate $10^3$ test set samples, making sure that the same combinations of \texttt{Galaxy\_name} attribute weren't shown in training and test to perform calibration of the inference model. The training of the NPE is done using the \texttt{sbi} backend, using 4 \texttt{nsf} (neural spline flow) with 10 layers and 100 neurons each. Once again we use a CNN as embedding for our observation $x^j$. The learning rate was set to ..., the batch size was set to .... The hyperparameter were tuned using the \texttt{optuna} package by maximizing the validation log likelihood.  
\end{enumerate}

The performance and calibration of the two inference pipeline are presented in the section \ref{sec:Analysis}.

\section{Surrogate Simulator}
