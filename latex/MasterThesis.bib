@article{bisterInferenceCosmicraySource2022,
  title = {Inference of Cosmic-Ray Source Properties by Conditional Invertible Neural Networks},
  author = {Bister, Teresa and Erdmann, Martin and K{\"o}the, Ullrich and Schulte, Josina},
  year = {2022},
  month = feb,
  journal = {The European Physical Journal C},
  volume = {82},
  number = {2},
  eprint = {2110.09493},
  primaryclass = {astro-ph, physics:hep-ex},
  pages = {171},
  issn = {1434-6044, 1434-6052},
  doi = {10.1140/epjc/s10052-022-10138-x},
  urldate = {2024-04-13},
  abstract = {The inference of physical parameters from measured distributions constitutes a core task in physics data analyses. Among recent deep learning methods, so-called conditional invertible neural networks provide an elegant approach owing to their probability-preserving bijective mapping properties. They enable training the parameter-observation correspondence in one mapping direction and evaluating the parameter posterior distributions in the reverse direction. Here, we study the inference of cosmic-ray source properties from cosmic-ray observations on Earth using extensive astrophysical simulations. We compare the performance of conditional invertible neural networks (cINNs) with the frequently used Markov Chain Monte Carlo (MCMC) method. While cINNs are trained to directly predict the parameters' posterior distributions, the MCMC method extracts the posterior distributions through a likelihood function that matches simulations with observations. Overall, we find good agreement between the physics parameters derived by the two different methods. As a result of its computational efficiency, the cINN method allows for a swift assessment of inference quality.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,High Energy Physics - Experiment},
  file = {C:\Users\viter\Zotero\storage\TXJT7EEK\Bister et al. - 2022 - Inference of cosmic-ray source properties by condi.pdf}
}

@article{buckOriginChemicalBimodality2020,
  title = {On the Origin of the Chemical Bimodality of Disk Stars: {{A}} Tale of Merger and Migration},
  shorttitle = {On the Origin of the Chemical Bimodality of Disk Stars},
  author = {Buck, Tobias},
  year = {2020},
  month = feb,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {491},
  number = {4},
  eprint = {1909.09162},
  primaryclass = {astro-ph},
  pages = {5435--5446},
  issn = {0035-8711, 1365-2966},
  doi = {10.1093/mnras/stz3289},
  urldate = {2024-03-06},
  abstract = {The Milky Way's stellar disk exhibits a bimodality in the [Fe/H] vs. [{$\alpha$}/Fe] plane, showing a distinct high-{$\alpha$} and low-{$\alpha$} sequence whose origin is still under debate. We examine the [Fe/H]-[{$\alpha$}/Fe] abundance plane in cosmological hydrodynamical simulations of Milky Way like galaxies from the NIHAO-UHD project and show that the bimodal {$\alpha$}-sequence is a generic consequence of a gas-rich merger at some time in the Galaxy's evolution. The high-{$\alpha$} sequence evolves first in the early galaxies, extending to high metallicities, while it is the low-{$\alpha$} sequence that is formed after the gas-rich merger. The merger brings in fresh metal-poor gas diluting the interstellar medium's metallicity while keeping the [{$\alpha$}/Fe] abundance almost unchanged. The kinematic, structural and spatial properties of the bimodal {$\alpha$}-sequence in our simulations reproduces that of observations. In all simulations, the high-{$\alpha$} disk is old, radially concentrated towards the galaxy's center and shows large scale heights. In contrast, the low-{$\alpha$} disk is younger, more radially extended and concentrated to the disk mid-plane. Our results show that the abundance plane is well described by these two populations that have been distributed radially across the disk by migration: at present-day in the solar neighbourhood, low-{$\alpha$} stars originate from both the inner and outer disk while most of the high-{$\alpha$} stars have migrated from the inner disk. We show that age dating the stars in the [Fe/H]-[{$\alpha$}/Fe] plane can constrain the time of the low-{$\alpha$} sequence forming merger and conclude that {$\alpha$}-bimodality is likely a not uncommon feature of disk galaxies.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Astrophysics - Astrophysics of Galaxies,Astrophysics - Solar and Stellar Astrophysics},
  file = {C:\Users\viter\Zotero\storage\4PVTSQH4\Buck - 2020 - On the origin of the chemical bimodality of disk s.pdf}
}

@article{buckOriginChemicalBimodality2020a,
  title = {On the Origin of the Chemical Bimodality of Disk Stars: {{A}} Tale of Merger and Migration},
  shorttitle = {On the Origin of the Chemical Bimodality of Disk Stars},
  author = {Buck, Tobias},
  year = {2020},
  month = feb,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {491},
  number = {4},
  eprint = {1909.09162},
  primaryclass = {astro-ph},
  pages = {5435--5446},
  issn = {0035-8711, 1365-2966},
  doi = {10.1093/mnras/stz3289},
  urldate = {2024-03-06},
  abstract = {The Milky Way's stellar disk exhibits a bimodality in the [Fe/H] vs. [{$\alpha$}/Fe] plane, showing a distinct high-{$\alpha$} and low-{$\alpha$} sequence whose origin is still under debate. We examine the [Fe/H]-[{$\alpha$}/Fe] abundance plane in cosmological hydrodynamical simulations of Milky Way like galaxies from the NIHAO-UHD project and show that the bimodal {$\alpha$}-sequence is a generic consequence of a gas-rich merger at some time in the Galaxy's evolution. The high-{$\alpha$} sequence evolves first in the early galaxies, extending to high metallicities, while it is the low-{$\alpha$} sequence that is formed after the gas-rich merger. The merger brings in fresh metal-poor gas diluting the interstellar medium's metallicity while keeping the [{$\alpha$}/Fe] abundance almost unchanged. The kinematic, structural and spatial properties of the bimodal {$\alpha$}-sequence in our simulations reproduces that of observations. In all simulations, the high-{$\alpha$} disk is old, radially concentrated towards the galaxy's center and shows large scale heights. In contrast, the low-{$\alpha$} disk is younger, more radially extended and concentrated to the disk mid-plane. Our results show that the abundance plane is well described by these two populations that have been distributed radially across the disk by migration: at present-day in the solar neighbourhood, low-{$\alpha$} stars originate from both the inner and outer disk while most of the high-{$\alpha$} stars have migrated from the inner disk. We show that age dating the stars in the [Fe/H]-[{$\alpha$}/Fe] plane can constrain the time of the low-{$\alpha$} sequence forming merger and conclude that {$\alpha$}-bimodality is likely a not uncommon feature of disk galaxies.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Astrophysics - Astrophysics of Galaxies,Astrophysics - Solar and Stellar Astrophysics},
  file = {C:\Users\viter\Zotero\storage\PK4PBQGE\Buck - 2020 - On the origin of the chemical bimodality of disk s.pdf}
}

@article{cunninghamReadingCARDsImprint2022,
  title = {Reading the {{CARDs}}: {{The Imprint}} of {{Accretion History}} in the {{Chemical Abundances}} of the {{Milky Way}}'s {{Stellar Halo}}},
  shorttitle = {Reading the {{CARDs}}},
  author = {Cunningham, Emily C. and Sanderson, Robyn E. and Johnston, Kathryn V. and Panithanpaisal, Nondh and Ness, Melissa K. and Wetzel, Andrew and Loebman, Sarah R. and Escala, Ivanna and Horta, Danny and {Faucher-Gigu{\`e}re}, Claude-Andr{\'e}},
  year = {2022},
  month = aug,
  journal = {The Astrophysical Journal},
  volume = {934},
  number = {2},
  pages = {172},
  issn = {0004-637X, 1538-4357},
  doi = {10.3847/1538-4357/ac78ea},
  urldate = {2024-03-06},
  abstract = {Abstract                            In the era of large-scale spectroscopic surveys in the Local Group, we can explore using chemical abundances of halo stars to study the star formation and chemical enrichment histories of the dwarf galaxy progenitors of the Milky Way (MW) and M31 stellar halos. In this paper, we investigate using the chemical abundance ratio distributions (CARDs) of seven stellar halos from the Latte suite of FIRE-2 simulations. We attempt to infer galaxies' assembly histories by modeling the CARDs of the stellar halos of the Latte galaxies as a linear combination of               template               CARDs from disrupted dwarfs, with different stellar masses               M               {$\star$}               and quenching times               t               100               . We present a method for constructing these templates using present-day dwarf galaxies. For four of the seven Latte halos studied in this work, we recover the mass spectrum of accreted dwarfs to a precision of {$<$}10\%. For the fraction of mass accreted as a function of               t               100               , we find the residuals of 20\%--30\% for five of the seven simulations. We discuss the failure modes of this method, which arise from the diversity of star formation and chemical enrichment histories that dwarf galaxies can take. These failure cases can be robustly identified by the high model residuals. Although the CARDs modeling method does not successfully infer the assembly histories in these cases, the CARDs of these disrupted dwarfs contain signatures of their unusual formation histories. Our results are promising for using CARDs to learn more about the histories of the progenitors of the MW and M31 stellar halos.},
  langid = {english},
  file = {C:\Users\viter\Zotero\storage\F2UXT3P9\Cunningham et al. - 2022 - Reading the CARDs The Imprint of Accretion Histor.pdf}
}

@misc{daxFlowMatchingScalable2023,
  title = {Flow {{Matching}} for {{Scalable Simulation-Based Inference}}},
  author = {Dax, Maximilian and Wildberger, Jonas and Buchholz, Simon and Green, Stephen R. and Macke, Jakob H. and Sch{\"o}lkopf, Bernhard},
  year = {2023},
  month = oct,
  number = {arXiv:2305.17161},
  eprint = {2305.17161},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-28},
  abstract = {Neural posterior estimation methods based on discrete normalizing flows have become established tools for simulation-based inference (SBI), but scaling them to high-dimensional problems can be challenging. Building on recent advances in generative modeling, we here present flow matching posterior estimation (FMPE), a technique for SBI using continuous normalizing flows. Like diffusion models, and in contrast to discrete flows, flow matching allows for unconstrained architectures, providing enhanced flexibility for complex data modalities. Flow matching, therefore, enables exact density evaluation, fast training, and seamless scalability to large architectures---making it ideal for SBI. We show that FMPE achieves competitive performance on an established SBI benchmark, and then demonstrate its improved scalability on a challenging scientific problem: for gravitational-wave inference, FMPE outperforms methods based on comparable discrete flows, reducing training time by 30\% with substantially improved accuracy. Our work underscores the potential of FMPE to enhance performance in challenging inference scenarios, thereby paving the way for more advanced applications to scientific problems.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\viter\Zotero\storage\T4VWPAIG\Dax et al. - 2023 - Flow Matching for Scalable Simulation-Based Infere.pdf}
}

@article{deasonUnravellingMassSpectrum2023,
  title = {Unravelling the Mass Spectrum of Destroyed Dwarf Galaxies with the Metallicity Distribution Function},
  author = {Deason, Alis J and Koposov, Sergey E and Fattahi, Azadeh and Grand, Robert J J},
  year = {2023},
  month = feb,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {520},
  number = {4},
  pages = {6091--6103},
  issn = {0035-8711, 1365-2966},
  doi = {10.1093/mnras/stad535},
  urldate = {2024-03-06},
  abstract = {ABSTRACT             Accreted stellar populations are comprised of the remnants of destroyed galaxies, and often dominate the `stellar haloes' of galaxies such as the Milky Way (MW). This ensemble of external contributors is a key indicator of the past assembly history of a galaxy. We introduce a novel statistical method that uses the unbinned metallicity distribution function (MDF) of a stellar population to estimate the mass spectrum of its progenitors. Our model makes use of the well-known mass--metallicity relation of galaxies and assumes Gaussian MDF distributions for individual progenitors: the overall MDF is thus a mixture of MDFs from smaller galaxies. We apply the method to the stellar halo of the MW, as well as the classical MW satellite galaxies. The stellar components of the satellite galaxies have relatively small sample sizes, but we do not find any evidence for accreted populations with L \&gt; Lhost/100. We find that the MW stellar halo has N {$\sim$} 1-3 massive progenitors (L {$\greaterequivlnt$} 108L{$\odot$}) within 10~kpc, and likely several hundred progenitors in total. We also test our method on simulations of MW-mass haloes, and find that our method is able to recover the true accreted population within a factor of 2. Future data sets will provide MDFs with orders of magnitude more stars, and this method could be a powerful technique to quantify the accreted populations down to the ultra-faint dwarf mass scale for both the MW and its satellites.},
  langid = {english},
  file = {C:\Users\viter\Zotero\storage\FJULZ94C\Deason et al. - 2023 - Unravelling the mass spectrum of destroyed dwarf g.pdf}
}

@misc{draxlerFreeformFlowsMake2024,
  title = {Free-Form {{Flows}}: {{Make Any Architecture}} a {{Normalizing Flow}}},
  shorttitle = {Free-Form {{Flows}}},
  author = {Draxler, Felix and Sorrenson, Peter and Zimmermann, Lea and Rousselot, Armand and K{\"o}the, Ullrich},
  year = {2024},
  month = apr,
  number = {arXiv:2310.16624},
  eprint = {2310.16624},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-05-08},
  abstract = {Normalizing Flows are generative models that directly maximize the likelihood. Previously, the design of normalizing flows was largely constrained by the need for analytical invertibility. We overcome this constraint by a training procedure that uses an efficient estimator for the gradient of the change of variables formula. This enables any dimension-preserving neural network to serve as a generative model through maximum likelihood training. Our approach allows placing the emphasis on tailoring inductive biases precisely to the task at hand. Specifically, we achieve excellent results in molecule generation benchmarks utilizing \$E(n)\$-equivariant networks. Moreover, our method is competitive in an inverse problem benchmark, while employing off-the-shelf ResNet architectures.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\viter\Zotero\storage\BFN25QGP\Draxler et al. - 2024 - Free-form Flows Make Any Architecture a Normalizi.pdf}
}

@misc{FrontierSimulationbasedInference,
  title = {The Frontier of Simulation-Based Inference},
  doi = {10.1073/pnas.1912789117},
  urldate = {2024-05-23},
  howpublished = {https://www.pnas.org/doi/10.1073/pnas.1912789117},
  langid = {english},
  file = {C\:\\Users\\viter\\Zotero\\storage\\WGQARZNV\\The frontier of simulation-based inference.pdf;C\:\\Users\\viter\\Zotero\\storage\\4KSU2YBR\\pnas.html}
}

@misc{galDropoutBayesianApproximation2016,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2016},
  month = oct,
  number = {arXiv:1506.02142},
  eprint = {1506.02142},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-05-22},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs --extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\viter\Zotero\storage\WBDU9K73\Gal and Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing .pdf}
}

@misc{heinrichHierarchicalNeuralSimulationBased2024,
  title = {Hierarchical {{Neural Simulation-Based Inference Over Event Ensembles}}},
  author = {Heinrich, Lukas and {Mishra-Sharma}, Siddharth and Pollard, Chris and Windischhofer, Philipp},
  year = {2024},
  month = feb,
  number = {arXiv:2306.12584},
  eprint = {2306.12584},
  primaryclass = {astro-ph, physics:hep-ex, stat},
  publisher = {arXiv},
  urldate = {2024-05-12},
  abstract = {When analyzing real-world data it is common to work with event ensembles, which comprise sets of observations that collectively constrain the parameters of an underlying model of interest. Such models often have a hierarchical structure, where ``local'' parameters impact individual events and ``global'' parameters influence the entire dataset. We introduce practical approaches for frequentist and Bayesian dataset-wide probabilistic inference in cases where the likelihood is intractable, but simulations can be realized via a hierarchical forward model. We construct neural estimators for the likelihood(-ratio) or posterior and show that explicitly accounting for the model's hierarchical structure can lead to significantly tighter parameter constraints. We ground our discussion using case studies from the physical sciences, focusing on examples from particle physics and cosmology.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Machine Learning,High Energy Physics - Experiment,Statistics - Machine Learning},
  file = {C:\Users\viter\Zotero\storage\44WW9URT\Heinrich et al. - 2024 - Hierarchical Neural Simulation-Based Inference Ove.pdf}
}

@misc{hoClassifierFreeDiffusionGuidance2022,
  title = {Classifier-{{Free Diffusion Guidance}}},
  author = {Ho, Jonathan and Salimans, Tim},
  year = {2022},
  month = jul,
  number = {arXiv:2207.12598},
  eprint = {2207.12598},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-03},
  abstract = {Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C:\Users\viter\Zotero\storage\LWZKJC3T\Ho and Salimans - 2022 - Classifier-Free Diffusion Guidance.pdf}
}

@misc{hoLtUILIAllinOneFramework2024,
  title = {{{LtU-ILI}}: {{An All-in-One Framework}} for {{Implicit Inference}} in {{Astrophysics}} and {{Cosmology}}},
  shorttitle = {{{LtU-ILI}}},
  author = {Ho, Matthew and Bartlett, Deaglan J. and Chartier, Nicolas and {Cuesta-Lazaro}, Carolina and Ding, Simon and Lapel, Axel and Lemos, Pablo and Lovell, Christopher C. and Makinen, T. Lucas and Modi, Chirag and Pandya, Viraj and Pandey, Shivam and Perez, Lucia A. and Wandelt, Benjamin and Bryan, Greg L.},
  year = {2024},
  month = feb,
  number = {arXiv:2402.05137},
  eprint = {2402.05137},
  primaryclass = {astro-ph},
  publisher = {arXiv},
  urldate = {2024-03-06},
  abstract = {This paper presents the Learning the Universe Implicit Likelihood Inference (LtU-ILI) pipeline, a codebase for rapid, user-friendly, and cutting-edge machine learning (ML) inference in astrophysics and cosmology. The pipeline includes software for implementing various neural architectures, training schema, priors, and density estimators in a manner easily adaptable to any research workflow. It includes comprehensive validation metrics to assess posterior estimate coverage, enhancing the reliability of inferred results. Additionally, the pipeline is easily parallelizable, designed for efficient exploration of modeling hyperparameters. To demonstrate its capabilities, we present real applications across a range of astrophysics and cosmology problems, such as: estimating galaxy cluster masses from X-ray photometry; inferring cosmology from matter power spectra and halo point clouds; characterising progenitors in gravitational wave signals; capturing physical dust parameters from galaxy colors and luminosities; and establishing properties of semi-analytic models of galaxy formation. We also include exhaustive benchmarking and comparisons of all implemented methods as well as discussions about the challenges and pitfalls of ML inference in astronomical sciences. All code and examples are made publicly available at https://github.com/maho3/ltu-ili.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Astrophysics - Astrophysics of Galaxies,Astrophysics - Cosmology and Nongalactic Astrophysics,Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Machine Learning},
  file = {C:\Users\viter\Zotero\storage\LUR69UF2\Ho et al. - 2024 - LtU-ILI An All-in-One Framework for Implicit Infe.pdf}
}

@misc{isobeExtendedFlowMatching2024,
  title = {Extended {{Flow Matching}}: A {{Method}} of {{Conditional Generation}} with {{Generalized Continuity Equation}}},
  shorttitle = {Extended {{Flow Matching}}},
  author = {Isobe, Noboru and Koyama, Masanori and Hayashi, Kohei and Fukumizu, Kenji},
  year = {2024},
  month = mar,
  number = {arXiv:2402.18839},
  eprint = {2402.18839},
  primaryclass = {cs, math},
  publisher = {arXiv},
  urldate = {2024-04-10},
  abstract = {The task of conditional generation is one of the most important applications of generative models, and numerous methods have been developed to date based on the celebrated diffusion models, with the guidance-based classifier-free method taking the lead. However, the theory of the guidance-based method not only requires the user to fine-tune the ``guidance strength'', but its target vector field does not necessarily correspond to the conditional distribution used in the training. In this paper, we develop the theory of conditional generation based on Flow Matching, a current strong contender of diffusion methods. Motivated by the interpretation of a probability path as a distribution on path space, we establish a novel theory of flow-based generation of conditional distribution by employing the mathematical framework of generalized continuity equation instead of the continuity equation in flow matching. This theory naturally derives a method that aims to match the matrix field as opposed to the vector field. Our framework ensures the continuity of the generated conditional distribution through the existence of flow between conditional distributions. We will present our theory through experiments and mathematical results.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {68T07 (Primary) 49Q22 (Secondary),Computer Science - Machine Learning,Mathematics - Analysis of PDEs,Mathematics - Functional Analysis,Mathematics - Optimization and Control,Mathematics - Probability},
  file = {C:\Users\viter\Zotero\storage\PRWQSSG3\Isobe et al. - 2024 - Extended Flow Matching a Method of Conditional Ge.pdf}
}

@misc{kingmaGlowGenerativeFlow2018,
  title = {Glow: {{Generative Flow}} with {{Invertible}} 1x1 {{Convolutions}}},
  shorttitle = {Glow},
  author = {Kingma, Diederik P. and Dhariwal, Prafulla},
  year = {2018},
  month = jul,
  number = {arXiv:1807.03039},
  eprint = {1807.03039},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-03-13},
  abstract = {Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1 {\texttimes} 1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realisticlooking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\viter\Zotero\storage\7KAKR5IG\Kingma and Dhariwal - 2018 - Glow Generative Flow with Invertible 1x1 Convolut.pdf}
}

@article{kravtsovGRUMPYSimpleFramework2022,
  title = {{{GRUMPY}}: A Simple Framework for Realistic Forward-Modelling of Dwarf Galaxies},
  shorttitle = {{{GRUMPY}}},
  author = {Kravtsov, Andrey and Manwadkar, Viraj},
  year = {2022},
  month = jun,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {514},
  number = {2},
  eprint = {2106.09724},
  primaryclass = {astro-ph},
  pages = {2667--2691},
  issn = {0035-8711, 1365-2966},
  doi = {10.1093/mnras/stac1439},
  urldate = {2024-05-22},
  abstract = {We present a simple regulator-type framework designed specifically for modelling formation of dwarf galaxies. Despite its simplicity, when coupled with realistic mass accretion histories of haloes from simulations and reasonable choices for model parameter values, the framework can reproduce a remarkably broad range of observed properties of dwarf galaxies over seven orders of magnitude in stellar mass. In particular, we show that the model can simultaneously match observational constraints on the stellar mass--halo mass relation, as well as observed relations between stellar mass and gas phase and stellar metallicities, gas mass, size, and star formation rate, as well as general form and diversity of star formation histories (SFHs) of observed dwarf galaxies. The model can thus be used to predict photometric properties of dwarf galaxies hosted by dark matter haloes in {$N$}-body simulations, such as colors, surface brightnesses, and mass-to-light ratios and to forward model observations of dwarf galaxies. We present examples of such modelling and show that colors and surface brightness distributions of model galaxies are in good agreement with observed distributions for dwarfs in recent observational surveys. We also show that in contrast with the common assumption, the absolute magnitude-halo mass relation is generally predicted to have a non-power law form in the dwarf regime, and that the fraction of haloes that host detectable ultrafaint galaxies is sensitive to reionization redshift ({$z$}rei) and is predicted to be consistent with observations for {$z$}rei 9.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Astrophysics - Astrophysics of Galaxies},
  file = {C:\Users\viter\Zotero\storage\89HGGK2A\2106.pdf}
}

@misc{millerTruncatedMarginalNeural2021,
  title = {Truncated {{Marginal Neural Ratio Estimation}}},
  author = {Miller, Benjamin Kurt and Cole, Alex and Forr{\'e}, Patrick and Louppe, Gilles and Weniger, Christoph},
  year = {2021},
  month = jun,
  eprint = {2107.01214},
  primaryclass = {astro-ph, physics:hep-ph, stat},
  doi = {10.5281/zenodo.5043706},
  urldate = {2024-05-21},
  abstract = {Parametric stochastic simulators are ubiquitous in science, often featuring high-dimensional input parameters and/or an intractable likelihood. Performing Bayesian parameter inference in this context can be challenging. We present a neural simulation-based inference algorithm which simultaneously offers simulation efficiency and fast empirical posterior testability, which is unique among modern algorithms. Our approach is simulation efficient by simultaneously estimating low-dimensional marginal posteriors instead of the joint posterior and by proposing simulations targeted to an observation of interest via a prior suitably truncated by an indicator function. Furthermore, by estimating a locally amortized posterior our algorithm enables efficient empirical tests of the robustness of the inference results. Since scientists cannot access the ground truth, these tests are necessary for trusting inference in real-world applications. We perform experiments on a marginalized version of the simulation-based inference benchmark and two complex and narrow posteriors, highlighting the simulator efficiency of our algorithm as well as the quality of the estimated marginal posteriors.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Machine Learning,High Energy Physics - Phenomenology,Statistics - Machine Learning},
  file = {C:\Users\viter\Zotero\storage\RQ3GIN9M\Miller et al. - 2021 - Truncated Marginal Neural Ratio Estimation.pdf}
}

@misc{montelDetectionTruncationStudying2022,
  title = {Detection Is Truncation: Studying Source Populations with Truncated Marginal Neural Ratio Estimation},
  shorttitle = {Detection Is Truncation},
  author = {Montel, Noemi Anau and Weniger, Christoph},
  year = {2022},
  month = nov,
  number = {arXiv:2211.04291},
  eprint = {2211.04291},
  primaryclass = {astro-ph},
  publisher = {arXiv},
  urldate = {2024-05-21},
  abstract = {Statistical inference of population parameters of astrophysical sources is challenging. It requires accounting for selection effects, which stem from the artificial separation between bright detected and dim undetected sources that is introduced by the analysis pipeline itself. We show that these effects can be modeled selfconsistently in the context of sequential simulation-based inference. Our approach couples source detection and catalog-based inference in a principled framework that derives from the truncated marginal neural ratio estimation (TMNRE) algorithm. It relies on the realization that detection can be interpreted as prior truncation. We outline the algorithm, and show first promising results.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics,Astrophysics - High Energy Astrophysical Phenomena,Astrophysics - Instrumentation and Methods for Astrophysics},
  file = {C:\Users\viter\Zotero\storage\HK3GLKPK\Montel and Weniger - 2022 - Detection is truncation studying source populatio.pdf}
}

@article{papamakariosNormalizingFlowsProbabilistic,
  title = {Normalizing {{Flows}} for {{Probabilistic Modeling}} and {{Inference}}},
  author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
  langid = {english},
  file = {C:\Users\viter\Zotero\storage\HSI9GT5S\Papamakarios et al. - Normalizing Flows for Probabilistic Modeling and I.pdf}
}

@inproceedings{pumarolaCFlowConditionalGenerative2020,
  title = {C-{{Flow}}: {{Conditional Generative Flow Models}} for {{Images}} and {{3D Point Clouds}}},
  shorttitle = {C-{{Flow}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Pumarola, Albert and Popov, Stefan and {Moreno-Noguer}, Francesc and Ferrari, Vittorio},
  year = {2020},
  month = jun,
  pages = {7946--7955},
  publisher = {IEEE},
  address = {Seattle, WA, USA},
  doi = {10.1109/CVPR42600.2020.00797},
  urldate = {2024-04-10},
  abstract = {Flow-based generative models have highly desirable properties like exact log-likelihood evaluation and exact latent-variable inference, however they are still in their infancy and have not received as much attention as alternative generative models. In this paper, we introduce C-Flow, a novel conditioning scheme that brings normalizing flows to an entirely new scenario with great possibilities for multimodal data modeling. C-Flow is based on a parallel sequence of invertible mappings in which a source flow guides the target flow at every step, enabling fine-grained control over the generation process. We also devise a new strategy to model unordered 3D point clouds that, in combination with the conditioning scheme, makes it possible to address 3D reconstruction from a single image and its inverse problem of rendering an image given a point cloud. We demonstrate our conditioning method to be very adaptable, being also applicable to image manipulation, style transfer and multi-modal image-to-image mapping in a diversity of domains, including RGB images, segmentation maps and edge masks.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-72817-168-5},
  langid = {english},
  file = {C:\Users\viter\Zotero\storage\8LDKNSQ9\Pumarola et al. - 2020 - C-Flow Conditional Generative Flow Models for Ima.pdf}
}

@misc{rouhiainenNormalizingFlowsRandom2021,
  title = {Normalizing Flows for Random Fields in Cosmology},
  author = {Rouhiainen, Adam and Giri, Utkarsh and M{\"u}nchmeyer, Moritz},
  year = {2021},
  month = may,
  number = {arXiv:2105.12024},
  eprint = {2105.12024},
  primaryclass = {astro-ph},
  publisher = {arXiv},
  urldate = {2024-04-09},
  abstract = {Normalizing flows are a powerful tool to create flexible probability distributions with a wide range of potential applications in cosmology. Here we are studying normalizing flows which represent cosmological observables at field level, rather than at the level of summary statistics such as the power spectrum. We evaluate the performance of different normalizing flows for both density estimation and sampling of near-Gaussian random fields, and check the quality of samples with different statistics such as power spectrum and bispectrum estimators. We explore aspects of these flows that are specific to cosmology, such as flowing from a physical prior distribution and evaluating the density estimation results in the analytically tractable correlated Gaussian case.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics,Astrophysics - Instrumentation and Methods for Astrophysics},
  file = {C:\Users\viter\Zotero\storage\5F9ARXGQ\Rouhiainen et al. - 2021 - Normalizing flows for random fields in cosmology.pdf}
}

@misc{sharrockSequentialNeuralScore2022,
  title = {Sequential {{Neural Score Estimation}}: {{Likelihood-Free Inference}} with {{Conditional Score Based Diffusion Models}}},
  shorttitle = {Sequential {{Neural Score Estimation}}},
  author = {Sharrock, Louis and Simons, Jack and Liu, Song and Beaumont, Mark},
  year = {2022},
  month = nov,
  number = {arXiv:2210.04872},
  eprint = {2210.04872},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-03-24},
  abstract = {We introduce Sequential Neural Posterior Score Estimation (SNPSE) and Sequential Neural Likelihood Score Estimation (SNLSE), two new score-based methods for Bayesian inference in simulator-based models. Our methods, inspired by the success of score-based methods in generative modelling, leverage conditional score-based diffusion models to generate samples from the posterior distribution of interest. These models can be trained using one of two possible objective functions, one of which approximates the score of the intractable likelihood, while the other directly estimates the score of the posterior. We embed these models into a sequential training procedure, which guides simulations using the current approximation of the posterior at the observation of interest, thereby reducing the simulation cost. We validate our methods, as well as their amortised, non-sequential variants, on several numerical examples, demonstrating comparable or superior performance to existing state-of-the-art methods such as Sequential Neural Posterior Estimation (SNPE) and Sequential Neural Likelihood Estimation (SNLE).},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\viter\Zotero\storage\H6VSZ3VY\Sharrock et al. - 2022 - Sequential Neural Score Estimation Likelihood-Fre.pdf}
}

@article{smithAstronomiaExMachina2023,
  title = {Astronomia Ex Machina: A History, Primer, and Outlook on Neural Networks in Astronomy},
  shorttitle = {Astronomia Ex Machina},
  author = {Smith, Michael J. and Geach, James E.},
  year = {2023},
  month = may,
  journal = {Royal Society Open Science},
  volume = {10},
  number = {5},
  eprint = {2211.03796},
  primaryclass = {astro-ph},
  pages = {221454},
  issn = {2054-5703},
  doi = {10.1098/rsos.221454},
  urldate = {2024-03-06},
  abstract = {In this review, we explore the historical development and future prospects of artificial intelligence (AI) and deep learning in astronomy. We trace the evolution of connectionism in astronomy through its three waves, from the early use of multilayer perceptrons, to the rise of convolutional and recurrent neural networks, and finally to the current era of unsupervised and generative deep learning methods. With the exponential growth of astronomical data, deep learning techniques offer an unprecedented opportunity to uncover valuable insights and tackle previously intractable problems. As we enter the anticipated fourth wave of astronomical connectionism, we argue for the adoption of GPT-like foundation models fine-tuned for astronomical applications. Such models could harness the wealth of high-quality, multimodal astronomical data to serve state-of-the-art downstream tasks. To keep pace with advancements driven by Big Tech, we propose a collaborative, open-source approach within the astronomy community to develop and maintain these foundation models, fostering a symbiotic relationship between AI and astronomy that capitalizes on the unique strengths of both fields.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Machine Learning},
  file = {C:\Users\viter\Zotero\storage\MFY33WYG\Smith and Geach - 2023 - Astronomia ex machina a history, primer, and outl.pdf}
}

@article{speiserDeepLearningEnables2021,
  title = {Deep Learning Enables Fast and Dense Single-Molecule Localization with High Accuracy},
  author = {Speiser, Artur and M{\"u}ller, Lucas-Raphael and Hoess, Philipp and Matti, Ulf and Obara, Christopher J. and Legant, Wesley R. and Kreshuk, Anna and Macke, Jakob H. and Ries, Jonas and Turaga, Srinivas C.},
  year = {2021},
  month = sep,
  journal = {Nature Methods},
  volume = {18},
  number = {9},
  pages = {1082--1090},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-021-01236-x},
  urldate = {2024-04-13},
  langid = {english},
  file = {C:\Users\viter\Zotero\storage\HHA2SAJL\Speiser et al. - 2021 - Deep learning enables fast and dense single-molecu.pdf}
}

@article{suFVAEsImproveVAEs,
  title = {F-{{VAEs}}: {{Improve VAEs}} with {{Conditional Flows}}},
  author = {Su, Jianlin and Wu, Guang},
  abstract = {In this paper, we integrate VAEs and flow-based generative models successfully and get f-VAEs. Compared with VAEs, f-VAEs generate more vivid images, solved the blurred-image problem of VAEs. Compared with flow-based models such as Glow, f-VAE is more lightweight and converges faster, achieving the same performance under smaller-size architecture.},
  langid = {english},
  file = {C:\Users\viter\Zotero\storage\LSDK299A\Su and Wu - f-VAEs Improve VAEs with Conditional Flows.pdf}
}

@article{tingDingHowManyElements2022,
  title = {How {{Many Elements Matter}}?},
  author = {Ting 丁, Yuan-Sen 源森 and Weinberg, David H.},
  year = {2022},
  month = mar,
  journal = {The Astrophysical Journal},
  volume = {927},
  number = {2},
  pages = {209},
  issn = {0004-637X, 1538-4357},
  doi = {10.3847/1538-4357/ac5023},
  urldate = {2024-03-06},
  abstract = {Abstract                            Some studies of stars' multielement abundance distributions suggest at least 5--7 significant dimensions, but others show that many elemental abundances can be predicted to high accuracy from [Fe/H] and [Mg/Fe] (or [Fe/H] and age) alone. We show that both propositions can be, and are, simultaneously true. We adopt a machine-learning technique known as normalizing flow to reconstruct the probability distribution of Milky Way disk stars in the space of 15 elemental abundances measured by APOGEE. Conditioning on               T               eff               and                                                                                                        log                                      g                                                                minimizes the differential systematics. After further conditioning on [Fe/H] and [Mg/Fe], the residual scatter for most abundances is               {$\sigma$}                                [                 X                 /H]                              {$\lessequivlnt$} 0.02 dex, consistent with APOGEE's reported statistical uncertainties of {$\sim$}0.01--0.015 dex and intrinsic scatter of 0.01--0.02 dex. Despite the small scatter, residual abundances display clear correlations between elements, which we show are too large to be explained by measurement uncertainties or by the finite sampling noise. We must condition on at least seven elements to reduce the correlations to a level consistent with the observational uncertainties. Our results demonstrate that cross-element correlations are a much more sensitive probe of a hidden structure than dispersion, and they can be measured precisely in a large sample even if the star-by-star measurement noise is comparable to the intrinsic scatter. We conclude that many elements have an independent story to tell, even for the               mundane               disk stars and elements produced by the core-collapse and Type Ia supernovae. The only way to learn these lessons is to measure the abundances directly, and not merely infer them.},
  langid = {english},
  file = {C:\Users\viter\Zotero\storage\SJT9AMQ2\Ting 丁 and Weinberg - 2022 - How Many Elements Matter.pdf}
}

@misc{tongImprovingGeneralizingFlowbased2024,
  title = {Improving and Generalizing Flow-Based Generative Models with Minibatch Optimal Transport},
  author = {Tong, Alexander and Fatras, Kilian and Malkin, Nikolay and Huguet, Guillaume and Zhang, Yanlei and {Rector-Brooks}, Jarrid and Wolf, Guy and Bengio, Yoshua},
  year = {2024},
  month = mar,
  number = {arXiv:2302.00482},
  eprint = {2302.00482},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-10},
  abstract = {Continuous normalizing flows (CNFs) are an attractive generative modeling technique, but they have been held back by limitations in their simulation-based maximum likelihood training. We introduce the generalized conditional flow matching (CFM) technique, a family of simulation-free training objectives for CNFs. CFM features a stable regression objective like that used to train the stochastic flow in diffusion models but enjoys the efficient inference of deterministic flow models. In contrast to both diffusion models and prior CNF training algorithms, CFM does not require the source distribution to be Gaussian or require evaluation of its density. A variant of our objective is optimal transport CFM (OT-CFM), which creates simpler flows that are more stable to train and lead to faster inference, as evaluated in our experiments. Furthermore, we show that when the true OT plan is available, our OT-CFM method approximates dynamic OT. Training CNFs with CFM improves results on a variety of conditional and unconditional generation tasks, such as inferring single cell dynamics, unsupervised image translation, and Schr{\"o}dinger bridge inference. The Python code is available at https://github.com/atong01/conditional-flow-matching.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {C:\Users\viter\Zotero\storage\C9ZIDACH\Tong et al. - 2024 - Improving and generalizing flow-based generative m.pdf}
}

@article{wangNIHAOProjectReproducing2015,
  title = {{{NIHAO}} Project {{I}}: {{Reproducing}} the Inefficiency of Galaxy Formation across Cosmic Time with a Large Sample of Cosmological Hydrodynamical Simulations},
  shorttitle = {{{NIHAO}} Project {{I}}},
  author = {Wang, Liang and Dutton, Aaron A. and Stinson, Gregory S. and Macci{\`o}, Andrea V. and Penzo, Camilla and Kang, Xi and Keller, Ben W. and Wadsley, James},
  year = {2015},
  month = nov,
  journal = {Monthly Notices of the Royal Astronomical Society},
  volume = {454},
  number = {1},
  eprint = {1503.04818},
  primaryclass = {astro-ph},
  pages = {83--94},
  issn = {0035-8711, 1365-2966},
  doi = {10.1093/mnras/stv1937},
  urldate = {2024-03-06},
  abstract = {We introduce project NIHAO (Numerical Investigation of a Hundred Astrophysical Objects), a set of 100 cosmological zoom-in hydrodynamical simulations performed using the gasoline code, with an improved implementation of the SPH algorithm. The haloes in our study range from dwarf (M200 {$\sim$} 5 {\texttimes} 109M{$\odot$}) to Milky Way (M200 {$\sim$} 2 {\texttimes} 1012M{$\odot$}) masses, and represent an unbiased sampling of merger histories, concentrations and spin parameters. The particle masses and force softenings are chosen to resolve the mass profile to below 1\% of the virial radius at all masses, ensuring that galaxy half-light radii are well resolved. Using the same treatment of star formation and stellar feedback for every object, the simulated galaxies reproduce the observed inefficiency of galaxy formation across cosmic time as expressed through the stellar mass vs halo mass relation, and the star formation rate vs stellar mass relation. We thus conclude that stellar feedback is the chief piece of physics required to limit the efficiency of star formation in galaxies less massive than the Milky Way.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Astrophysics - Astrophysics of Galaxies,Astrophysics - Cosmology and Nongalactic Astrophysics},
  file = {C:\Users\viter\Zotero\storage\3D5NFAMT\Wang et al. - 2015 - NIHAO project I Reproducing the inefficiency of g.pdf}
}

@article{wangSBIFlexibleUltrafast2023,
  title = {{{SBI}}++: {{Flexible}}, {{Ultra-fast Likelihood-free Inference Customized}} for {{Astronomical Applications}}},
  shorttitle = {{{SBI}}++},
  author = {Wang, Bingjie and Leja, Joel and Villar, V. Ashley and Speagle, Joshua S.},
  year = {2023},
  month = jul,
  journal = {The Astrophysical Journal Letters},
  volume = {952},
  number = {1},
  eprint = {2304.05281},
  primaryclass = {astro-ph},
  pages = {L10},
  issn = {2041-8205, 2041-8213},
  doi = {10.3847/2041-8213/ace361},
  urldate = {2024-03-06},
  abstract = {Flagship near-future surveys targeting 108 - 109 galaxies across cosmic time will soon reveal the processes of galaxy assembly in unprecedented resolution. This creates an immediate computational challenge on effective analyses of the full data-set. With simulation-based inference (SBI), it is possible to attain complex posterior distributions with the accuracy of traditional methods but with a {$>$} 104 increase in speed. However, it comes with a major limitation. Standard SBI requires the simulated data to have characteristics identical to those of the observed data, which is often violated in astronomical surveys due to inhomogeneous coverage and/or fluctuating sky and telescope conditions. In this work, we present a complete SBI-based methodology, ``SBI++,'' for treating out-of-distribution measurement errors and missing data. We show that out-of-distribution errors can be approximated by using standard SBI evaluations and that missing data can be marginalized over using SBI evaluations over nearby data realizations in the training set. In addition to the validation set, we apply SBI++ to galaxies identified in extragalactic images acquired by the James Webb Space Telescope, and show that SBI++ can infer photometric redshifts at least as accurately as traditional sampling methods---and crucially, better than the original SBI algorithm using training data with a wide range of observational errors. SBI++ retains the fast inference speed of {$\sim$} 1 sec for objects in the observational training set distribution, and additionally permits parameter inference outside of the trained noise and data at {$\sim$} 1 min per object. This expanded regime has broad implications for future applications to astronomical surveys a).},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Astrophysics - Astrophysics of Galaxies,Astrophysics - Instrumentation and Methods for Astrophysics},
  file = {C:\Users\viter\Zotero\storage\9ST6MYCD\Wang et al. - 2023 - SBI++ Flexible, Ultra-fast Likelihood-free Infere.pdf}
}

@misc{winklerLearningLikelihoodsConditional2023,
  title = {Learning {{Likelihoods}} with {{Conditional Normalizing Flows}}},
  author = {Winkler, Christina and Worrall, Daniel and Hoogeboom, Emiel and Welling, Max},
  year = {2023},
  month = nov,
  number = {arXiv:1912.00042},
  eprint = {1912.00042},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-10},
  abstract = {Normalizing Flows (NFs) are able to model complicated distributions pY (y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density pZ(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities pY {\textbar}X (y{\textbar}x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\viter\Zotero\storage\675UQ6F7\Winkler et al. - 2023 - Learning Likelihoods with Conditional Normalizing .pdf}
}

@misc{wolfGalacticFlowLearningGeneralized2023,
  title = {{{GalacticFlow}}: {{Learning}} a {{Generalized Representation}} of {{Galaxies}} with {{Normalizing Flows}}},
  shorttitle = {{{GalacticFlow}}},
  author = {Wolf, Luca and Buck, Tobias},
  year = {2023},
  month = dec,
  number = {arXiv:2312.06017},
  eprint = {2312.06017},
  primaryclass = {astro-ph},
  publisher = {arXiv},
  urldate = {2024-03-06},
  abstract = {State-of-the-art galaxy formation simulations generate data within weeks or months. Their results consist of a random sub-sample of possible galaxies with a fixed number of stars. We propose a ML based method, GalacticFlow, that generalizes such results. We use normalizing flows to learn the extended distribution function of galaxies conditioned on global galactic parameters. GalacticFlow then provides a continuized and condensed representation of the ensemble of galaxies in the data. Thus, essentially compressing large amounts of explicit simulation data into a small implicit generative model. Our model is able to evaluate any galaxy eDF given by a set of global parameters and allows generating arbitrarily many stars from it. We show that we can learn such a representation, embodying the entire mass range from dwarf to Milky Way mass, from only 90 galaxies in {$\sim$} 18 hours on a single RTX 2080Ti and generate a new galaxy of one million stars within a few seconds.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Astrophysics - Astrophysics of Galaxies},
  file = {C:\Users\viter\Zotero\storage\863WTSII\Wolf and Buck - 2023 - GalacticFlow Learning a Generalized Representatio.pdf}
}

@article{xuDenoisingDiffusionProbabilistic2023,
  title = {Denoising {{Diffusion Probabilistic Models}} to {{Predict}} the {{Density}} of {{Molecular Clouds}}},
  author = {Xu, Duo and Tan, Jonathan C. and Hsu, Chia-Jung and Zhu, Ye},
  year = {2023},
  month = jun,
  journal = {The Astrophysical Journal},
  volume = {950},
  number = {2},
  pages = {146},
  issn = {0004-637X, 1538-4357},
  doi = {10.3847/1538-4357/accae5},
  urldate = {2024-03-24},
  abstract = {We introduce the state-of-the-art deep-learning denoising diffusion probabilistic model as a method to infer the volume or number density of giant molecular clouds (GMCs) from projected mass surface density maps. We adopt magnetohydrodynamic simulations with different global magnetic field strengths and large-scale dynamics, i.e., noncolliding and colliding GMCs. We train a diffusion model on both mass surface density maps and their corresponding mass-weighted number density maps from different viewing angles for all the simulations. We compare the diffusion model performance with a more traditional empirical two-component and three-component power-law fitting method and with a more traditional neural network machine-learning approach. We conclude that the diffusion model achieves an order-of-magnitude improvement on the accuracy of predicting number density compared to that by other methods. We apply the diffusion method to some example astronomical column density maps of Taurus and the infrared dark clouds G28.37+0.07 and G35.39-0.33 to produce maps of their mean volume densities.},
  langid = {english},
  file = {C:\Users\viter\Zotero\storage\KC683KGB\Xu et al. - 2023 - Denoising Diffusion Probabilistic Models to Predic.pdf}
}
